{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9btj5uSAGeZnX/ow8ugQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysuseel/MiniProject_AD/blob/main/contrastive_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qp-88oiJ7WP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import json\n",
        "import requests\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONFIGURABLE VARIABLES"
      ],
      "metadata": {
        "id": "Ze5cNKDaKymP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GITHUB_REPO = \"https://raw.githubusercontent.com/ajaysuseel/MiniProject_AD/main/ajay/\"\n",
        "JSON_FILE = \"contrastive_captions.json\"\n",
        "IMAGES_FOLDER = \"images/\""
      ],
      "metadata": {
        "id": "ZJrVXqSGKz0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking model modules"
      ],
      "metadata": {
        "id": "oIkzZDSLOSO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your BLIP model\n",
        "model_name = \"Salesforce/blip-image-captioning-base\"  # or your chosen variant\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Iterate over the model's modules\n",
        "for name, module in model.named_modules():\n",
        "    # Optionally, you can filter names containing 'attn' or 'proj'\n",
        "    if \"attn\" in name.lower() or \"proj\" in name.lower():\n",
        "        print(name)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AwgN0LgHNvaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(model)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KJgvire_QPng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load BLIP Model with LoRA"
      ],
      "metadata": {
        "id": "ARM1-4X-K-d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_blip_with_lora():\n",
        "    model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "    print(\"Loading BLIP-1 model with LoRA...\")\n",
        "    processor = BlipProcessor.from_pretrained(model_name)\n",
        "    model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    target_modules=[f\"vision_model.encoder.layers.{i}.self_attn.qkv\" for i in range(12)]\n",
        "\n",
        "    # Apply LoRA (PEFT)\n",
        "    lora_config = LoraConfig(\n",
        "        r=8, lora_alpha=16, lora_dropout=0.1, target_modules=target_modules\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "6-t8eB_8LCsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset"
      ],
      "metadata": {
        "id": "Ls6i_JXsLFkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    json_url = GITHUB_REPO + JSON_FILE\n",
        "    try:\n",
        "        response = requests.get(json_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        print(f\"Loaded {len(data)} image-caption pairs.\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "YnE74huxLJKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Dataset Class"
      ],
      "metadata": {
        "id": "LOsLJLMDLMJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "class ContrastiveCaptionDataset(Dataset):\n",
        "    def __init__(self, data, processor, images_dir):\n",
        "        self.data = data\n",
        "        self.processor = processor\n",
        "        self.images_dir = images_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_url = self.images_dir + item[\"filename\"]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {item['filename']}: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Tokenize positive caption using the 'pos_caption' key\n",
        "        pos_encoding = self.processor(\n",
        "            text=item[\"pos_caption\"],\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "        pos_encoding = {key: val.squeeze(0) for key, val in pos_encoding.items()}\n",
        "\n",
        "        # Tokenize negative caption using the 'neg_caption' key\n",
        "        neg_encoding = self.processor(\n",
        "            text=item[\"neg_caption\"],\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "        neg_encoding = {key: val.squeeze(0) for key, val in neg_encoding.items()}\n",
        "\n",
        "        # Set the labels for contrastive loss:\n",
        "        # 'pos_labels' from the positive encoding and 'neg_labels' from the negative encoding\n",
        "        pos_encoding[\"pos_labels\"] = pos_encoding[\"input_ids\"]\n",
        "        pos_encoding[\"neg_labels\"] = neg_encoding[\"input_ids\"]\n",
        "\n",
        "        return pos_encoding\n"
      ],
      "metadata": {
        "id": "P9d9soG5LO7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create DataLoader"
      ],
      "metadata": {
        "id": "svXGq0_SLMGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(data, processor, batch_size=4):\n",
        "    dataset = ContrastiveCaptionDataset(data, processor, GITHUB_REPO + IMAGES_FOLDER)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        if len(batch) == 0:\n",
        "            return None\n",
        "        keys = batch[0].keys()\n",
        "        return {key: torch.stack([b[key] for b in batch]) for key in keys}\n",
        "\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "qCHb5c1oLXry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contrastive Loss Function"
      ],
      "metadata": {
        "id": "OA1YP--RLbgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(image_embeds, pos_text_embeds, neg_text_embeds, temperature=0.07):\n",
        "    sim_pos = torch.cosine_similarity(image_embeds, pos_text_embeds, dim=-1)\n",
        "    sim_neg = torch.cosine_similarity(image_embeds, neg_text_embeds, dim=-1)\n",
        "    loss = -torch.log(torch.exp(sim_pos / temperature) / (torch.exp(sim_pos / temperature) + torch.exp(sim_neg / temperature)))\n",
        "    return loss.mean()"
      ],
      "metadata": {
        "id": "NoMrvm69Lek-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-Tune BLIP with Contrastive Loss"
      ],
      "metadata": {
        "id": "vWypg1GrLgiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_blip_contrastive(model, dataloader, num_epochs=3, learning_rate=5e-5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(f\"ðŸš€ Starting fine-tuning on {device} for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            pos_input_ids = batch[\"pos_labels\"].to(device)\n",
        "            neg_input_ids = batch[\"neg_labels\"].to(device)\n",
        "\n",
        "            # Generate embeddings\n",
        "\n",
        "            image_embeds = model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "            # Pass embeddings through the BERT model to get last_hidden_state\n",
        "            pos_outputs = model.text_decoder.bert(inputs_embeds=model.text_decoder.bert.embeddings(pos_input_ids))\n",
        "            neg_outputs = model.text_decoder.bert(inputs_embeds=model.text_decoder.bert.embeddings(neg_input_ids))\n",
        "\n",
        "            pos_text_embeds = pos_outputs.last_hidden_state.mean(dim=1)\n",
        "            neg_text_embeds = neg_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "\n",
        "            loss = contrastive_loss(image_embeds, pos_text_embeds, neg_text_embeds)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"âœ… Epoch {epoch+1} completed | Average Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    model.save_pretrained(\"./models/finetuned_blip1\")\n",
        "    processor.save_pretrained(\"./models/finetuned_blip1\")\n",
        "    print(\"ðŸŽ¯ Fine-tuning complete and model saved!\")"
      ],
      "metadata": {
        "id": "R-CsanOlLi0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Execution"
      ],
      "metadata": {
        "id": "bPmMeDIfLluu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model, processor = load_blip_with_lora()\n",
        "    data = load_dataset()\n",
        "\n",
        "    if not data:\n",
        "        print(\"No data found. Exiting.\")\n",
        "    else:\n",
        "        dataloader = create_dataloader(data, processor)\n",
        "        if dataloader is None:\n",
        "            print(\"Error: No valid data samples found. Exiting.\")\n",
        "        else:\n",
        "            train_blip_contrastive(model, dataloader,20)"
      ],
      "metadata": {
        "id": "DAvCJSA7LoiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "ho90wOkPUl8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
      ],
      "metadata": {
        "id": "2xIMe8nlUn-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ajaysuseel/MiniProject_AD.git"
      ],
      "metadata": {
        "id": "QH4RlMzHoJZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONFIGURABLE VARIABLES"
      ],
      "metadata": {
        "id": "ErnZdQtTUu57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_FOLDER = \"/content/MiniProject_AD/ajay/images\"\n",
        "GROUND_TRUTH_JSON = \"/content/MiniProject_AD/ajay/captions.json\"\n",
        "MODEL_PATH = \"./models/finetuned_blip1\"  # Path to your fine-tuned model"
      ],
      "metadata": {
        "id": "5haV9xjbUpzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading datset and model"
      ],
      "metadata": {
        "id": "CPzRfWmNUySh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "def load_ground_truth(local_json_path):\n",
        "    \"\"\"\n",
        "    Load ground truth captions from a local JSON file.\n",
        "    The JSON should be a dictionary mapping image filenames to captions.\n",
        "\n",
        "    Parameters:\n",
        "      local_json_path (str): Local file path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "      dict: A dictionary with keys as image filenames and values as captions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(local_json_path, \"r\") as f:\n",
        "            gt_data = json.load(f)\n",
        "        print(f\"Loaded {len(gt_data)} ground truth captions from {local_json_path}.\")\n",
        "        return gt_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ground truth captions: {e}\")\n",
        "        return {}\n",
        "\n",
        "def load_image(image_path):\n",
        "    \"\"\"\n",
        "    Open an image from a local file path and convert it to RGB.\n",
        "\n",
        "    Parameters:\n",
        "      image_path (str): The local file path to the image.\n",
        "\n",
        "    Returns:\n",
        "      PIL.Image or None: The loaded image in RGB mode, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "# Load Fine-Tuned BLIP Model and Processor\n",
        "\n",
        "def load_model_and_processor(model_path):\n",
        "    \"\"\"\n",
        "    Loads the fine-tuned BLIP model and its processor from a given directory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        processor = BlipProcessor.from_pretrained(model_path)\n",
        "        model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        print(f\"Model loaded on {device}.\")\n",
        "        return model, processor, device\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None, None"
      ],
      "metadata": {
        "id": "ZHJgMdEEU-lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Captions"
      ],
      "metadata": {
        "id": "mhKEqD57VLMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(model, processor, device, image):\n",
        "    \"\"\"\n",
        "    Given an image, generate a caption using the fine-tuned model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(**inputs)\n",
        "        caption = processor.decode(output_ids[0], skip_special_tokens=True)\n",
        "        return caption\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "hw227rvKVNKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_with_captions(image_path, gt_caption, generated_caption, bleu_score):\n",
        "    image = load_image(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Cannot display image: {image_path}\")\n",
        "        return\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    title_text = f\"GT: {gt_caption}\\nGen: {generated_caption}\\nBLEU: {bleu_score:.4f}\"\n",
        "    plt.title(title_text, fontsize=10)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0To0AJca_zch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating the finetuned model"
      ],
      "metadata": {
        "id": "qBqwFP5LVN8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def evaluate_model(image_folder, gt_json_path, model_path):\n",
        "    gt_captions = load_ground_truth(gt_json_path)\n",
        "    if not gt_captions:\n",
        "        print(\"No ground truth data available. Exiting evaluation.\")\n",
        "        return\n",
        "\n",
        "    model, processor, device = load_model_and_processor(model_path)\n",
        "    if model is None:\n",
        "        print(\"Model loading failed. Exiting evaluation.\")\n",
        "        return\n",
        "\n",
        "    generated_captions = {}\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    print(f\"Found {len(image_files)} images in {image_folder}.\")\n",
        "\n",
        "    for filename in tqdm(image_files, desc=\"Evaluating images\"):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = load_image(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "        caption = generate_caption(model, processor, device, image)\n",
        "        generated_captions[filename] = caption\n",
        "\n",
        "    individual_scores = {}\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    for filename, gt_caption in gt_captions.items():\n",
        "        if filename in generated_captions:\n",
        "            gen_caption = generated_captions[filename]\n",
        "            hypothesis = gen_caption.split()\n",
        "            reference = [gt_caption.split()]  # BLEU expects a list of references\n",
        "            score = sentence_bleu(reference, hypothesis)\n",
        "            individual_scores[filename] = score\n",
        "            references.append(reference)\n",
        "            hypotheses.append(hypothesis)\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            display_image_with_captions(image_path, gt_caption, gen_caption, score)\n",
        "        else:\n",
        "            print(f\"Warning: No generated caption for {filename}\")\n",
        "\n",
        "    avg_bleu = corpus_bleu(references, hypotheses)\n",
        "    print(\"\\n--- Evaluation Summary ---\")\n",
        "    for filename, score in individual_scores.items():\n",
        "        print(f\"{filename}: BLEU Score = {score:.4f}\")\n",
        "    print(f\"\\nAverage Corpus BLEU Score: {avg_bleu:.4f}\")\n"
      ],
      "metadata": {
        "id": "gmhT_D-TVSV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Clone your repository if needed:\n",
        "    # !git clone https://github.com/ajaysuseel/MiniProject_AD.git\n",
        "\n",
        "    # Fine-tuning Phase:\n",
        "    with open(os.path.join(LOCAL_REPO_PATH, \"captions.json\"), \"r\") as f:\n",
        "        fine_tuning_data = json.load(f)\n",
        "    print(f\"Loaded {len(fine_tuning_data)} fine-tuning samples.\")\n",
        "\n",
        "    # Load BLIP-2 model with LoRA and processor for fine-tuning\n",
        "    model, processor = load_blip2_with_lora()\n",
        "    dataloader = create_dataloader(fine_tuning_data, processor, batch_size=2)\n",
        "\n",
        "    # # Fine-tune the model using contrastive fine-tuning\n",
        "    # model = train_blip_contrastive(model, dataloader, num_epochs=3, learning_rate=5e-5)\n",
        "\n",
        "    # Evaluate the fine-tuned model\n",
        "    evaluate_model(IMAGE_FOLDER, GROUND_TRUTH_JSON, MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "PNm2waq5WklP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eGvjDHac-9QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WpvLK6U4-_ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}