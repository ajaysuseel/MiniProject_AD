{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8KXg6Ua8dN+pppdHlVVcT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysuseel/MiniProject_AD/blob/main/hazard_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6h456LSGtv0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predefined hazard keywords for simple hazard detection"
      ],
      "metadata": {
        "id": "ia7w2WF-G9Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HAZARD_KEYWORDS = {\n",
        "    \"flood\": [\"flood\", \"water\", \"inundated\"],\n",
        "    \"fallen_tree\": [\"fallen tree\", \"tree\"],\n",
        "    \"debris\": [\"debris\", \"rubble\"],\n",
        "    \"damage\": [\"damaged\", \"cracked\", \"broken\"]\n",
        "}"
      ],
      "metadata": {
        "id": "fronbNw-G55x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading captions and model"
      ],
      "metadata": {
        "id": "phcCnFTNHVYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ground_truth(local_json_path):\n",
        "    try:\n",
        "        with open(local_json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "        # Convert the list into a dictionary: {filename: description}\n",
        "        gt_data = {item[\"filename\"]: item[\"description\"] for item in data}\n",
        "        print(f\"Loaded {len(gt_data)} ground truth captions from {local_json_path}.\")\n",
        "        return gt_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ground truth captions: {e}\")\n",
        "        return {}\n",
        "def load_image(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "def load_model_and_processor(model_path):\n",
        "    try:\n",
        "        processor = BlipProcessor.from_pretrained(model_path, ignore_mismatched_sizes=True)\n",
        "        model = BlipForConditionalGeneration.from_pretrained(model_path, ignore_mismatched_sizes=True)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        print(f\"Model loaded on {device}.\")\n",
        "        return model, processor, device\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None, None\n"
      ],
      "metadata": {
        "id": "5dMrbAnSHGS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Caption for an Image"
      ],
      "metadata": {
        "id": "R_Xn4sTEHgFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(model, processor, device, image):\n",
        "    try:\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(**inputs)\n",
        "        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        return caption\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "IJnGHR88HblT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Detect Hazard in a Caption"
      ],
      "metadata": {
        "id": "FpkrRF-RHtLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_hazard(caption, hazard_keywords):\n",
        "    caption_lower = caption.lower()\n",
        "    detected = []\n",
        "    for hazard, keywords in hazard_keywords.items():\n",
        "        for kw in keywords:\n",
        "            if kw in caption_lower:\n",
        "                detected.append(hazard)\n",
        "                break  # Found one keyword for this hazard; move to the next hazard.\n",
        "    if detected:\n",
        "        return \", \".join(detected)\n",
        "    else:\n",
        "        return \"No Hazard Detected\""
      ],
      "metadata": {
        "id": "qYqddJV0HjOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Display Image with Captions, BLEU Score, and Hazard Detection Result"
      ],
      "metadata": {
        "id": "ZppAdXpsH8sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_with_results(image_path, gt_caption, generated_caption, bleu_score, hazard_result):\n",
        "    image = load_image(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Cannot display image: {image_path}\")\n",
        "        return\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    title_text = f\"GT: {gt_caption}\\nGen: {generated_caption}\\nBLEU: {bleu_score:.4f}\\nHazard: {hazard_result}\"\n",
        "    plt.title(title_text, fontsize=10)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ygKiZmFJHwxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate Hazard Detection Pipeline"
      ],
      "metadata": {
        "id": "dHNwDwi1IHvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_hazard_detection(image_folder, gt_json_path, model_path, hazard_keywords):\n",
        "    # Load ground truth captions\n",
        "    gt_captions = load_ground_truth(gt_json_path)\n",
        "    if not gt_captions:\n",
        "        print(\"No ground truth data available. Exiting evaluation.\")\n",
        "        return\n",
        "\n",
        "    # Load the fine-tuned model and processor\n",
        "    model, processor, device = load_model_and_processor(model_path)\n",
        "    if model is None:\n",
        "        print(\"Model loading failed. Exiting evaluation.\")\n",
        "        return\n",
        "\n",
        "    generated_captions = {}\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    print(f\"Found {len(image_files)} images in {image_folder}.\")\n",
        "\n",
        "    # Generate captions for each image\n",
        "    for filename in tqdm(image_files, desc=\"Evaluating images\"):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = load_image(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "        caption = generate_caption(model, processor, device, image)\n",
        "        generated_captions[filename] = caption\n",
        "\n",
        "    individual_scores = {}\n",
        "    references = []\n",
        "    hypotheses = {}\n",
        "\n",
        "    # Evaluate using BLEU and run hazard detection\n",
        "    for filename, gt_caption in gt_captions.items():\n",
        "        if filename in generated_captions:\n",
        "            gen_caption = generated_captions[filename]\n",
        "            hypothesis = gen_caption.split()\n",
        "            reference = [gt_caption.split()]  # BLEU expects a list of reference tokens\n",
        "            score = sentence_bleu(reference, hypothesis)\n",
        "            individual_scores[filename] = score\n",
        "            references.append(reference)\n",
        "            hypotheses[filename] = hypothesis\n",
        "\n",
        "            # Detect hazards in the generated caption\n",
        "            hazard_result = detect_hazard(gen_caption, hazard_keywords)\n",
        "\n",
        "            # Display the image with captions, BLEU score, and hazard detection result\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "            display_image_with_results(image_path, gt_caption, gen_caption, score, hazard_result)\n",
        "        else:\n",
        "            print(f\"Warning: No generated caption for {filename}\")\n",
        "\n",
        "    avg_bleu = corpus_bleu(references, list(hypotheses.values()))\n",
        "    print(\"\\n--- Evaluation Summary ---\")\n",
        "    for filename, score in individual_scores.items():\n",
        "        print(f\"{filename}: BLEU Score = {score:.4f}\")\n",
        "    print(f\"\\nAverage Corpus BLEU Score: {avg_bleu:.4f}\")"
      ],
      "metadata": {
        "id": "eXdG62p3H_ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Execution"
      ],
      "metadata": {
        "id": "XgxSHocxIOUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configurable Variables"
      ],
      "metadata": {
        "id": "WkLvd4YWIVrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOCAL_REPO_PATH = \"/content/MiniProject_AD/abhiram\"  # Local path to your cloned repo\n",
        "IMAGE_FOLDER = os.path.join(LOCAL_REPO_PATH, \"images\")\n",
        "GROUND_TRUTH_JSON = os.path.join(LOCAL_REPO_PATH, \"captions.json\")  # Ground truth captions file\n",
        "MODEL_SAVE_PATH = \"./models/finetuned_blip1\"  # Path where your fine-tuned model is saved"
      ],
      "metadata": {
        "id": "ZOjuSVvqIU3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    evaluate_hazard_detection(IMAGE_FOLDER, GROUND_TRUTH_JSON, MODEL_SAVE_PATH, HAZARD_KEYWORDS)"
      ],
      "metadata": {
        "id": "ZK1RjzycINRQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}