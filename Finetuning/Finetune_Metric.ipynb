{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import requests\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "2wX4sSuepqmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CONFIGURABLE VARIABLES"
      ],
      "metadata": {
        "id": "xFHxZZ6GAAXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurable variables\n",
        "GITHUB_REPO = \"https://raw.githubusercontent.com/ajaysuseel/MiniProject_AD/main/data/\"\n",
        "JSON_FILE = \"captions.json\"\n",
        "IMAGES_FOLDER = \"images/\""
      ],
      "metadata": {
        "id": "xfgt5wmDq4_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function: Load BLIP Model"
      ],
      "metadata": {
        "id": "gM8MuRrK_7CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_blip_model():\n",
        "    model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "    processor = BlipProcessor.from_pretrained(model_name)\n",
        "    model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "ENCspCL5pudw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function: Load Dataset from GitHub"
      ],
      "metadata": {
        "id": "dOHFVGlc_25O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    json_url = GITHUB_REPO + JSON_FILE\n",
        "    try:\n",
        "        response = requests.get(json_url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        print(f\"Loaded {len(data)} image-caption pairs.\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "NNFr0FoBp0E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom Dataset Class"
      ],
      "metadata": {
        "id": "a7Kv_RYqADIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionDataset(Dataset):\n",
        "    def __init__(self, data, processor, images_dir):\n",
        "        self.data = data\n",
        "        self.processor = processor\n",
        "        self.images_dir = images_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_url = self.images_dir + item[\"filename\"]\n",
        "\n",
        "        try:\n",
        "            image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {item['filename']}: {e}\")\n",
        "            return None\n",
        "\n",
        "        encoding = self.processor(\n",
        "            text=item[\"description\"],\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        encoding[\"labels\"] = encoding[\"input_ids\"]\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "ekxPgAhVmQQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create DataLoader"
      ],
      "metadata": {
        "id": "qlOFU4ORAFee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(data, processor, batch_size=4):\n",
        "    dataset = CaptionDataset(data, processor, GITHUB_REPO + IMAGES_FOLDER)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        if len(batch) == 0:\n",
        "            return None\n",
        "        keys = batch[0].keys()\n",
        "        return {key: torch.stack([b[key] for b in batch]) for key in keys}\n",
        "\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "GoQN6K8Mngj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "vj5ByzOcAH0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_blip(model, dataloader, num_epochs=3, learning_rate=5e-5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(f\"ðŸš€ Starting fine-tuning on {device} for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            optimizer.zero_grad()\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1} completed | Average Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    model.save_pretrained(\"./models/finetuned_blip1\")\n",
        "    processor.save_pretrained(\"./models/finetuned_blip1\")\n",
        "    print(\"Fine-tuning complete and model saved!\")"
      ],
      "metadata": {
        "id": "kSLcO1l7n6t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "iQ1Gc4z7odAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model, processor = load_blip_model()\n",
        "    data = load_dataset()\n",
        "\n",
        "    if not data:\n",
        "        print(\"No data found. Exiting.\")\n",
        "    else:\n",
        "        dataloader = create_dataloader(data, processor)\n",
        "        if dataloader is None:\n",
        "            print(\"Error: No valid data samples found. Exiting.\")\n",
        "        else:\n",
        "            train_blip(model, dataloader, num_epochs=20)\n"
      ],
      "metadata": {
        "id": "8qW5QT8FoDuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gdrive"
      ],
      "metadata": {
        "id": "1GJPjsfrAGZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define source & destination paths\n",
        "MODELS_SRC = \"/content/models\"\n",
        "GDRIVE_DEST = \"/content/drive/MyDrive/models\"\n",
        "\n",
        "# Copy models folder to Google Drive\n",
        "if os.path.exists(MODELS_SRC):\n",
        "    !cp -r {MODELS_SRC} {GDRIVE_DEST}\n",
        "    print(f\"Models folder successfully copied to Google Drive: {GDRIVE_DEST}\")\n",
        "else:\n",
        "    print(\"No 'models' folder found in /content/. Please check your path.\")\n"
      ],
      "metadata": {
        "id": "-1lHO6c_W3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "POSrYSzKbyPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source (Google Drive folder)\n",
        "src_folder = \"/content/drive/MyDrive/model\"\n",
        "\n",
        "# Destination (Colab models folder)\n",
        "dest_folder = \"/content/models\"\n",
        "\n",
        "# # Remove existing folder if it exists\n",
        "# if os.path.exists(dest_folder):\n",
        "#     shutil.rmtree(dest_folder)\n",
        "#     print(\"Existing 'models' folder deleted.\")\n",
        "\n",
        "# Copy the folder from Drive\n",
        "shutil.copytree(src_folder, dest_folder)\n",
        "print(\"Models folder copied successfully from Drive!\")\n"
      ],
      "metadata": {
        "id": "GI2OP5HVYITJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "HU-28RLHb5f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "J_02bg6BrYJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pZEL3hGNvu0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content')  # Change to a known existing directory\n",
        "\n",
        "!git clone https://github.com/ajaysuseel/MiniProject_AD.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LerjmwWncNlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_FOLDER = \"/content/MiniProject_AD/raw_data/pranav/images\"  # Image folder\n",
        "CAPTIONS_FILE = \"/content/MiniProject_AD/raw_data/pranav/captions.json\"  # JSON with filename-description\n",
        "MODEL_PATH = \"./models/finetuned_blip1\"  # Fine-tuned BLIP model path"
      ],
      "metadata": {
        "id": "AP1p4NjfrenY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_processor(model_path):\n",
        "    try:\n",
        "        processor = BlipProcessor.from_pretrained(model_path)\n",
        "        model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        print(f\"Model loaded on {device}\")\n",
        "        return model, processor, device\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None, None, None\n",
        "def load_ground_truth(local_json_path):\n",
        "    try:\n",
        "        with open(local_json_path, \"r\") as f:\n",
        "            gt_data = json.load(f)\n",
        "            gt_data = {item[\"filename\"]: item[\"description\"] for item in gt_data}\n",
        "        print(f\"Loaded {len(gt_data)} ground truth captions.\")\n",
        "        return gt_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ground truth: {e}\")\n",
        "        return {}\n",
        "def load_image(image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "JpQPv5-Arf2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(model, processor, device, image):\n",
        "    try:\n",
        "        inputs = processor(images=image, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(**inputs)\n",
        "        return processor.decode(output_ids[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "E_nSzo26rrWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for metrics - run only if they are not available\n",
        "!pip install rouge-score\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "h9kk209f26rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for cider\n",
        "#!git clone https://github.com/tylin/coco-caption\n",
        "#!pip install -e coco-caption\n"
      ],
      "metadata": {
        "id": "iRawjm044TSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for meteor\n",
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "id": "CcGFJrCH6eaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_image_with_captions(image_path, gt_caption, generated_caption, bleu_score, meteor_score, rouge_score):\n",
        "    image = load_image(image_path)\n",
        "    if image is None:\n",
        "        return\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"GT: {gt_caption}\\nGen: {generated_caption}\\nBLEU: {bleu_score:.4f} | METEOR: {meteor_score:.4f} | ROUGE: {rouge_score:.4f}\", fontsize=10)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "d7Al3t1F6RA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge_score import rouge_scorer\n",
        "# from pycocoevalcap.cider.cider import Cider  # Commented out due to cider installation issues\n",
        "from collections import defaultdict\n",
        "\n",
        "def evaluate_model(image_folder, gt_json_path, model_path):\n",
        "    gt_captions = load_ground_truth(gt_json_path)\n",
        "    if not gt_captions:\n",
        "        print(\"No ground truth data available. Exiting evaluation.\")\n",
        "        return\n",
        "\n",
        "    model, processor, device = load_model_and_processor(model_path)\n",
        "    if model is None:\n",
        "        print(\"Model loading failed. Exiting evaluation.\")\n",
        "        return\n",
        "\n",
        "    generated_captions = {}\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "    meteor_scores = []\n",
        "    rouge_scores = []\n",
        "    # cider_scores = []  # Commented out CIDEr\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    print(f\"ðŸ”¹ Found {len(image_files)} images in {image_folder}.\")\n",
        "\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "    for filename in tqdm(image_files, desc=\"Evaluating Images\"):\n",
        "        if filename not in gt_captions:\n",
        "            continue\n",
        "\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = load_image(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        gen_caption = generate_caption(model, processor, device, image)\n",
        "        generated_captions[filename] = gen_caption\n",
        "\n",
        "        hypothesis = gen_caption.split()\n",
        "        reference = [gt_captions[filename].split()]\n",
        "\n",
        "        # BLEU Score\n",
        "        bleu_score = sentence_bleu(reference, hypothesis)\n",
        "\n",
        "        # METEOR Score\n",
        "        meteor = meteor_score([gt_captions[filename].split()], gen_caption.split())\n",
        "        meteor_scores.append(meteor)\n",
        "\n",
        "        # ROUGE Score\n",
        "        rouge = scorer.score(gt_captions[filename], gen_caption)[\"rougeL\"].fmeasure\n",
        "        rouge_scores.append(rouge)\n",
        "\n",
        "        # Store for Corpus BLEU calculation\n",
        "        references.append(reference)\n",
        "        hypotheses.append(hypothesis)\n",
        "\n",
        "        # Display Image with Captions & Scores\n",
        "        display_image_with_captions(image_path, gt_captions[filename], gen_caption, bleu_score, meteor, rouge)\n",
        "\n",
        "    # Compute Corpus Scores\n",
        "    corpus_bleu_score = corpus_bleu(references, hypotheses)\n",
        "    avg_meteor_score = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
        "    avg_rouge_score = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0\n",
        "\n",
        "    print(\"\\n--- Evaluation Summary ---\")\n",
        "    print(f\"Corpus BLEU Score: {corpus_bleu_score:.4f}\")\n",
        "    print(f\"Average METEOR Score: {avg_meteor_score:.4f}\")\n",
        "    print(f\"Average ROUGE-L Score: {avg_rouge_score:.4f}\")\n",
        "\n",
        "    # CIDEr Code\n",
        "    # cider_scorer = Cider()\n",
        "    # cider_score, _ = cider_scorer.compute_score(references, hypotheses)\n",
        "    # print(f\"CIDEr Score: {cider_score:.4f}\")  # Commented out\n",
        "\n"
      ],
      "metadata": {
        "id": "J4D_zr_L2sQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(IMAGE_FOLDER, CAPTIONS_FILE, MODEL_PATH)"
      ],
      "metadata": {
        "id": "AYt0cLEX5e_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}