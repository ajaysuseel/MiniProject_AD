\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{array}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{url}

\usepackage{titlesec}
\usepackage{tocloft}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem} % Add this in your preamble
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage[hidelinks]{hyperref}
\usepackage{minitoc}
\usepackage[table]{xcolor}
\usepackage{fancyhdr} % For header and footer customization
\pagestyle{fancy}     % Use fancy page style
\fancyhf{}            % Clear all header and footer fields
\fancyfoot[C]{\thepage} 
\renewcommand{\headrulewidth}{0pt}



\usepackage{titling}


% Page geometry - matching original document
\geometry{
    left=1in,
    right=1in,
    top=1in,
    bottom=1in
}

% Line spacing
\onehalfspacing

% Remove default page numbering for custom placement


% Custom page numbering
\usepackage{lastpage}
\usepackage{fancyhdr}

% Title formatting to match original
\titleformat{\section}
{\normalfont\Large\bfseries\centering}{}{0em}{}

\titleformat{\subsection}
{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% Hyperref setup
\hypersetup{
    colorlinks=false,
    linkcolor=black,
    filecolor=black,
    urlcolor=black,
    citecolor=black
}



\begin{document}

% Custom commands
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\newpage
\thispagestyle{empty}

% Declaration Page - exactly as in original
\section*{DECLARATION}

\vspace{1em}

We undersigned hereby declare that the project report on "Vision Language Model Based Out Of Label Detection In Autonomous Driving", submitted as part of our curriculum, Mini Project under APJ Abdul Kalam Technological University, Kerala is a bonafide work done by us under the supervision of Dr. Dimple A Shajahan, Project Coordinator and Professor in the Department of Computer Science and Engineering, Prof. Febeena M, Assistant Professor, Department of Computer Science and Engineering, TKMCE.

\vspace{1em}

This submission represents our ideas in our own words and from other sources that have been adequately and accurately cited and referenced. We also declare that we have adhered to ethics of academic honesty and integrity and have not misrepresented or fabricated any data or idea or fact or source in our submission.

\vspace{1em}

We understand that any violation of the above will be a cause for disciplinary action by the institute and/or the University and can also evoke penal action from the sources which have thus not been properly cited or from whom proper permission has not been obtained. This report has not been previously formed as the basis for the award of any degree, diploma or similar title of any other University.

\vspace{2cm}

\begin{flushright}
\begin{tabular}{c}
Place: Kollam \\
Date: 28/05/2025 \\[1cm]
Mr. Abhiram K Aravind \\[0.25cm]
Mr. Ajay Suseel \\[0.25cm]
Mr. Kishan P K \\[0.25cm]
Mr. Pranav I B
\end{tabular}
\end{flushright}

\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}
% Acknowledgement Page - exactly as in original
\section*{ACKNOWLEDGEMENT}

\vspace{1em}

We take this opportunity to express our deep sense of gratitude to the Almighty and sincere thanks to all who helped me to complete the project successfully.

\vspace{1em}

We sincerely thank Dr. Sajeeb R, Principal, TKM College of Engineering, for providing us with all the necessary facilities and support for the project.

\vspace{1em}

We are extremely grateful to Dr. Dimple A Shajahan, Project Coordinator and Professor, Department of Computer Science and Engineering, along with Prof. Febeena M , Assistant Professor, Department of Computer Science and Engineering for their constructive guidance, advice, constant support and technical guidance provided throughout the making of this project. Without their intellectual support and apt suggestions at the perfect time, this project work would not be possible.

\vspace{1em}

We extend our immense gratitude to all Faculties and Technical Staffs in the Department of Computer Science and Engineering, for their help and necessary facilities to complete the project. Our humble gratitude and heartiest thanks also go to our parents and friends, who have supported and helped us on the course of this work.

\vspace{2cm}

\begin{flushright}
\begin{tabular}{l}
Mr. Abhiram K Aravind \\[0.25cm]
Mr. Ajay Suseel \\[0.25cm]
Mr. Kishan P K \\[0.25cm]
Mr. Pranav I B
\end{tabular}
\end{flushright}

\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}
% Abstract Page - exactly as in original
\section*{ABSTRACT}

\vspace{1em}

Out-of-label hazards pose a significant safety risk to autonomous driving systems. This project aims to develop a comprehensive framework for detecting such hazards by leveraging a fine-tuned BLIP model and a robust hazard detection mechanism based on generated scene captions. The BLIP model is first fine-tuned on the DHPR dataset images , annotated with hazard aware captions , to generate captions that describe potential hazards in driving scenes. These captions are then analyzed using a triple approach: keyword extraction, semantic similarity assessment against a set of predefined base cases and a Graph neural network trained on image captions classifying it into low,medium or high severity levels.

\vspace{1em}

Through this methodology, the system seeks to identify out-of-label hazards that may not have been explicitly seen during training. By incorporating semantic similarity techniques and Graph Neural Networks , the proposed approach enhances the generalizability and explainability of hazard detection in autonomous driving, contributing to safer and more reliable AI-driven navigation.

\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}
% Contents Page - exactly as in original
\section*{CONTENTS}
\vspace{1em}

\begin{tabularx}{\textwidth}{@{}Xr@{}}
\textbf{1 \quad Introduction} & 2 \\
\quad 1.1 \quad Motivation \dotfill & 3 \\
\quad 1.2 \quad Objectives \dotfill & 3 \\
\quad 1.3 \quad Organization of report \dotfill & 4 \\
\\
\textbf{2 \quad Literature Review} \dotfill & 5 \\
\\
\textbf{3 \quad Proposed System Design} & 6 \\
\quad 3.1 \quad Caption Generation \dotfill & 6 \\
\quad 3.2 \quad Feature extraction and Knowledge graph \dotfill & 7 \\
\quad 3.3 \quad Hazard Classification \dotfill & 7 \\
\\
\quad 3.4 \quad Explainable Hazard Detection \dotfill & 7 \\
\\
\textbf{4 \quad Implementation} & 8 \\
\quad 4.1 \quad Caption Generation \dotfill & 8 \\
\quad 4.2 \quad Feature extraction and Graph based reasoning \dotfill & 9 \\
\quad 4.3 \quad Hazard Classification \dotfill & 9 \\
\\
\textbf{5 \quad Testing} & 10 \\
\quad 5.1 \quad Testing scope \dotfill & 10 \\
\quad 5.2 \quad Test cases \dotfill & 11 \\
\quad 5.3 \quad Testing Environment \dotfill & 11 \\
\quad 5.4 \quad Test data \dotfill & 12 \\
\\
\textbf{6 \quad Result} \dotfill & 13 \\
\quad 6.1 \quad Caption Generation \dotfill & 13 \\
\quad 6.2 \quad Hazard Detection \dotfill & 13 \\
\quad 6.3 \quad Explainable Hazard Detection \dotfill & 13 \\

\\
\textbf{7 \quad Evaluation} \dotfill & 14 \\
\quad 7.1 \quad Caption Generation Evaluation \dotfill & 14 \\
\quad 7.2 \quad Hazard Detection Evaluation \dotfill & 14 \\
\\
\textbf{8 \quad Conclusion and Future scope} \dotfill & 15 \\
\\
\textbf{9 \quad References} \dotfill & 16 \\
\end{tabularx}


\newpage

\thispagestyle{empty}

\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}
\section*{List of Figures}

\begin{table}[htbp]
  \centering
  \renewcommand{\arraystretch}{1.2}  % increase row height
  \begin{tabularx}{\textwidth}{|c|c|X|c|}
    \hline
    \textbf{Sl No} 
      & \textbf{Figure} 
      & \textbf{Title} 
      & \textbf{Page no} \\
    \hline
    1 & Fig 1 
      & Overall System Architecture
      & 23 \\ 
    \hline
        \hline
    2 & Fig 2 
      & Architecture of Hazard Classifier 
      & 23 \\ 
    \hline
        \hline
    3 & Fig 3 
      & Sample caption generation 1
      & 23 \\ 
    \hline
        \hline
    4 & Fig 4 
      & Sample caption generation 2
      & 23 \\ 
    \hline
        \hline
    5 & Fig 5 
      & Low severity hazard detection
      & 23 \\ 
    \hline
        \hline
    6 & Fig 6 
      & High severity hazard detection
      & 23 \\ 
    \hline
        \hline
    7 & Fig 7 
      & Medium severity hazard detection
      & 23 \\ 
    \hline
        \hline
        
   8 & Fig 8 
      & Attention over nodes in high severity hazard detection
      & 23 \\ 
    \hline
        \hline     
    9 & Fig 9
      & Confusion matrix
      & 23 \\ 
    \hline
        \hline
    10 & Fig 10
      & Classification report
      & 23 \\ 
    \hline
        11 & Fig 11
      & System Architecture Diagram
      & 23 \\ 
    \hline
        12 & Fig 12
      & Architecture of GraphSAGE Classifier
      & 23 \\ 
    \hline
    
    % add more rows here as needed...
  \end{tabularx}
\end{table}




\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}

\begin{center}
    {\LARGE \textbf{Appendices}} \\[1.5ex]
    
    \begin{tabular*}{\textwidth}{| c |@{\extracolsep{\fill}} l |}
        \hline
        \textbf{Sl No} & \textbf{Appendix} \\ 
        \hline
         1             &  SRS document \\ 
        \hline
         2             & SDD document \\ 
        \hline
         3             & Record of Changes \\ 
        \hline
         4             & Fulfillment of COs and POs \\ 
        \hline
         5             & Project Timeline \\ 
        \hline
    \end{tabular*}
\end{center}

\newpage
\thispagestyle{empty}
\begin{center}
    {\LARGE \textbf{Abbreviations}} \\[1.5ex]
\begin{tabular}{|p{3cm}|p{8cm}|}
  \hline
  \textbf{Abbreviations} & \textbf{Full form} \\ 
  \hline
  VLM & Vision Language Model \\ 
  \hline
  BLIP   & Bootstrapping Language – Image Pre-training \\ 
  \hline
  GNN  & Graph Neural Network\\ 
  \hline

  
    BLEU   & Bilingual Evaluation Understudy \\ 
  \hline
  ROUGE  & Recall ‐ Oriented Understudy for Gisting Evaluation \\ 
  \hline
  METEOR & Metric for Evaluation of Translation with Explicit Ordering \\ 
  \hline

  GPU  & Graphics Processing Unit \\ 
  \hline

\end{tabular}
\end{center}


\newpage

\setcounter{page}{1}
% Chapter 1 - Introduction
\section*{1. Introduction}

\vspace{1em}

Autonomous driving refers to the capability of a vehicle to perceive its surroundings, make decisions, and navigate without direct human intervention. A critical component of any autonomous driving system (ADS) is the detection and interpretation of potential hazards. Timely and accurate hazard detection is essential for ensuring safety and making proactive driving decisions.

\vspace{1em}

While traditional hazard detection systems have made significant progress using supervised learning methods, they largely depend on labeled datasets with predefined categories. This reliance presents a major limitation when dealing with out-of-distribution (OOD) or "out-of-label" hazards - unusual, rare, or previously unseen scenarios that fall outside the scope of the system's training data. Examples may include novel obstacle types, unexpected human behaviors, or environmental anomalies.

\vspace{1em}

The ability to detect such unknown or rare hazards is increasingly recognized as a critical challenge in the development of robust ADS. However, existing models often struggle with generalizing beyond their training labels. This has motivated ongoing research into novel representations and learning strategies that can improve generalization and reasoning under uncertainty.

\subsection*{1.1 Motivation}

\vspace{1em}

Autonomous driving systems have the potential to significantly reduce road accidents by enabling rapid, data-driven decision-making. While considerable progress has been made, achieving full autonomy remains an unsolved challenge, particularly in handling the wide range of complex, real-world driving scenarios.

\vspace{1em}

A key limitation lies in the widespread reliance on supervised learning, which restricts models to predefined labels and scenarios seen during training. This makes it difficult for current systems to recognize and respond to novel or rare hazards - such as unusual objects, unexpected behaviors, or unfamiliar environments.

\vspace{1em}

To build safer and more adaptable autonomous systems, there is a growing need for richer and more flexible scene understanding. Generating natural language descriptions of visual scenes enable models to capture high-level semantic information that goes beyond fixed object labels. These descriptions can provide deeper context for decision-making, improving the system's ability to detect hazards and respond to unfamiliar scenarios.

\subsection*{1.2 Objectives}
\vspace{1em}
\begin{itemize}
    \item To construct a dataset of road scene images paired with detailed, hazard-aware captions that emphasize contextual and safety-relevant information.
    
    \item To fine-tune a vision-language model (VLM) to generate context-rich natural language descriptions of driving scenes, focusing on accuracy and inclusion of hazard-related cues.
    
    \item To develop a hazard classification model that analyzes the generated captions and assigns a hazard severity rating (e.g., low, medium, high) based on the semantic content of the description.
\end{itemize}


\subsection*{1.3 Organization of the report}

\vspace{1em}

This report is organized into several sections, each focusing on a key aspect of the hazard detection system for autonomous driving using image captioning.The report begins with an introduction and literature review, discussing the challenges of detecting out-of-label hazards in autonomous driving systems and reviewing existing approaches in hazard detection, image captioning, and vision-language models.

\vspace{1em}

The methodology section details the creation of a dataset consisting of hazard-enriched image-caption pairs, followed by the fine-tuning of a vision-language model (BLIP) to generate detailed captions. It also covers the development of a hazard classification model that uses these captions to assess the severity of detected hazards.

\vspace{1em}

The implementation section provides an overview of the system's architecture, highlighting the caption generation process and hazard classification mechanism using Graph Neural Network. It also discusses the tools and  frameworks used during development.

\vspace{1em}

The testing and evaluation section evaluates the performance of both the captioning and classification models, including metrics and tests to assess accuracy, hazard detection capabilities, and overall system reliability.
\vspace{1em}

Finally, the results section shows the various results including the performance and results obtained on previously unseen data.

\newpage

% Chapter 2 - Literature Review
\section*{2. Literature review}

\vspace{1em}

The detection of hazards in autonomous driving is a critical challenge that has garnered significant attention in recent years. Traditional approaches often rely on supervised learning, which is limited by the availability and diversity of labeled data. As a result, detecting novel or rare hazards—those that fall outside predefined categories—remains a major hurdle. Recent advancements in image captioning and vision-language models (VLMs) have shown potential in addressing these challenges by providing richer, more flexible scene understanding. Studies have explored various techniques, such as text-to-image generation and multimodal fusion, to improve hazard detection accuracy.

\vspace{1em}

In Dannier Xiao et.al 's work on "HazardVLM: A Video Language Model for Real-Time Hazard Description in Automated Driving Systems" provides valuable insights on the topic of using VLMs to generate detailed captions for road scenes. Such research highlights the need of context rich captions for better scene understanding and decision making.

\vspace{1em}

In William Hamilton et al.'s work on "Inductive Representation Learning on Large Graphs," the authors introduce GraphSAGE, a framework that enables learning node representations that generalize to unseen data. This inductive approach is useful in dynamic environments like autonomous driving, where new hazards may appear. The study shows how graph-based reasoning can support better generalization and context-aware classification.

\vspace{1em}

In their paper "BLIP: Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding and Generation," Junnan Li et al. propose a vision-language framework that combines contrastive learning, image-text matching, and language modeling to learn strong visual-semantic representations. A key innovation is their bootstrapping approach, which filters noisy web captions by generating cleaner descriptions to improve pretraining. In our work, we build upon the pretrained BLIP model and fine-tune it on a hazard-focused road scene dataset to generate detailed, context-aware captions that are more relevant to autonomous driving scenarios.

\vspace{1em}

Despite the progress made in autonomous driving and hazard detection research, there are still some research gaps that need to be addressed. Limited studies have explored the use of vision-language models for detecting and classifying complex, out-of-label hazards in real-world driving scenarios. Even fewer have incorporated graph-based reasoning for deeper contextual understanding. The present project, with its focus on generating descriptive captions using a fine-tuned BLIP model, analyzing semantic similarity and keyword presence, and classifying hazards into low, medium, and high severity using a graph neural network trained on a custom knowledge graph, aligns with these underexplored areas and offers the potential to significantly improve hazard awareness and safety in autonomous vehicles.

\newpage

% Chapter 3 - Proposed System Design
\section*{3. Proposed System Design}

The proposed system for out-of-label hazard detection follows a multi-stage pipeline combining vision-language model, knowledge graph reasoning, and graph-based classification. The overall goal is to generate meaningful captions from input images and use those captions to infer potential hazards including novel hazards. 

\vspace{1em}

\begin{figure}[h]
    \centering
    \includegraphics[height=1.7cm,width=1\textwidth]{Figuras/image.png} % adjust width as needed
    \caption{Overall System Architecture}
    \label{fig1}
\end{figure}




\subsection*{3.1 Caption Generation}

The first part of the system uses a fine-tuned BLIP model to generate detailed captions from road scene images. This model has been finetuned on a dataset focused on road hazards, so the captions it produces are more likely to include relevant and safety-critical information. These captions help describe what's happening in the scene in a way that highlights possible risks.

\subsection*{3.2 Feature extraction and Knowledge graph}

Next, the captions are analyzed using three methods. First, a keyword matching module looks for specific hazard-related words and assigns scores based on their severity. Second, a semantic similarity module checks how closely the meaning of the caption matches known hazard phrases using sentence embeddings. Third, a graph-based score is calculated by connecting the caption to a knowledge graph, which is built from common patterns in the training captions. This graph includes relationships between objects and actions, and links similar words together using synonyms, allowing the system to understand deeper context.

\subsection*{3.3 Hazard Classification}

In the final step, the system uses a Graph Neural Network (GNN) to reason over the knowledge graph. It learns from how different elements in the graph are connected, especially those related to the caption. An attention mechanism selects the most relevant parts of the graph, and this information is combined with the earlier features to form a single input. This is passed through a simple neural network to predict whether the hazard is low, medium, or high. By combining language, structure, and reasoning, the system can detect even those hazards it has not seen before.

\begin{figure}[h]
    \centering
    \includegraphics[height=6cm,width=1\textwidth]{Figuras/image2.png} % adjust width as needed
    \caption{Architecture of Hazard Classifier}
    \label{fig2}
\end{figure}
\subsection*{3.4 Explainable Hazard Detection}
Attention over nodes is a mechanism in graph‐based models that learns to assign each node a normalized weight reflecting its importance for a given task: after computing node embeddings, a small attention network scores each node and applies a softmax so that all weights sum to one. By ranking nodes according to these learned attention weights, the model highlights which objects or relationships in the graph contributed most to its prediction. Because one can visualize or list the highest ‐ weighted nodes during inference, it becomes transparent which parts of the input drove the final decision. Consequently, when the model outputs a prediction — such as a hazard level — users can inspect the top ‐ attention nodes to understand exactly which features in the scene were most influential, turning what would otherwise be a black‐box output into an explainable decision.

\newpage

% Chapter 4 - Implementation
\section*{4. Implementation}

\vspace{1em}

\subsection*{4.1 Caption Generation (BLIP Fine-Tuning)}

The caption generation model is based on the Salesforce/blip-image-captioning-base architecture from HuggingFace. It is fine-tuned to produce hazard-relevant captions from road scene images.



\begin{itemize}
    \item \textbf{Model}: \texttt{BlipForConditionalGeneration}
    
    \item \textbf{Processor}: \texttt{BlipProcessor} from HuggingFace
    
    \item \textbf{Loss Function}: Cross-entropy loss between predicted and ground-truth token sequences
    
    \item \textbf{Optimizer}: AdamW
    
    \item \textbf{Learning Rate}: 5e-5
    
    \item \textbf{Freezing Strategy}:
    \begin{itemize}[label=\textopenbullet,leftmargin=2cm]
        \item The vision embeddings (\texttt{vision\_model.embeddings}) are fully frozen.
        \item The first $n = 6$ layers of the vision encoder (\texttt{vision\_model.encoder.layers}) are also frozen to retain general visual representations.
    \end{itemize}
    
    \item \textbf{Finetuning Framework}: PyTorch Lightning
    
    \item \textbf{Hardware}: GPU-accelerated (CUDA support)
\end{itemize}

\subsection*{4.2 Feature extraction and Graph based reasoning}

Once the captions are generated, they are analyzed using three distinct heuristic modules to extract semantic cues about potential hazards:

1. Keyword Matching:
\begin{itemize}[leftmargin=1cm, label={}]
\item a. Uses predefined lists of hazard-related keywords classified by severity (low, medium, high).
\item b. Each detected keyword contributes a weighted score based on its severity level.
\end{itemize}

2. Graph Context Score:
\begin{itemize}[leftmargin=1cm, label={}]
\item a. Caption triplets (subject, relation, object) are extracted and mapped onto a knowledge graph.
\item b. The graph is constructed from frequent patterns in the training data.
\item c. Node centrality and connectivity are used to assign context-aware scores.
\end{itemize}

3. Semantic Similarity:
\begin{itemize}[leftmargin=1cm, label={}]
\item a. Captions are encoded using sentence-transformers.
\item b. Cosine similarity is computed between caption embeddings and a set of known hazard phrase embeddings.
\end{itemize}

\subsection*{4.3 Hazard Classification (Graph Neural Network)}

The final classification module uses a GraphSAGE-based GNN to reason over the knowledge graph and predict the severity of potential hazards.

\begin{itemize}
    \item \textbf{GNN Model}: GraphSAGE with 3 layers
    
    \item \textbf{Hidden Dimension}: 128
    
    \item \textbf{Attention Mechanism}: Used to weigh relevant nodes in the graph based on their relation to the caption
    
    \item \textbf{Dropout}: 0.3
\end{itemize}

The GNN output is concatenated with the 9D heuristic feature vector and passed through a Multi-Layer Perceptron (MLP) for classification:

\begin{itemize}
    \item \textbf{MLP Classifier}:
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item \textbf{Input}: (128 + 9) features
        \item \textbf{Architecture}: Linear $\rightarrow$ LayerNorm $\rightarrow$ ReLU $\rightarrow$ Dropout $\rightarrow$ Linear
        \item \textbf{Output}: 3-class prediction (Low, Medium, High hazard)
    \end{itemize}
    
    \item \textbf{Loss Function}: Cross-entropy
    
    \item \textbf{Optimizer}: AdamW

     \item \textbf{Learning Rate}: 1e-3
\end{itemize}

The GNN-based classifier, enriched with both structural graph context and heuristic features, enables generalization to previously unseen hazard types and improves interpretability.


\newpage


% Chapter 5 - Testing
\section*{5. Testing}

\vspace{1em}

Testing the model involves evaluating the system's ability to identify and classify hazards in unseen road scene images. The testing covers both the caption generation (BLIP) and hazard classification (GNN) modules, ensuring robustness and accuracy of the full pipeline.

\subsection*{5.1 Testing scope}



\begin{itemize}
    
    \item \textbf{Performance Metrics:}
\begin{itemize}[label=\textopenbullet, leftmargin=1.5cm]
    \item \textbf{Caption Quality:} Evaluated using BLEU, ROUGE, and METEOR scores.
    \item \textbf{Classification Performance:} Measured using Accuracy, Precision, Recall, and F1-score.
\end{itemize}

    
    \item \textbf{Hyperparameter Validation:}
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item \textbf{BLIP}: Tested with different learning rates and frozen/unfrozen configurations.
        \item \textbf{GNN}: Evaluated across epochs, dropout rates, and hidden layer dimensions.
    \end{itemize}
    
    \item \textbf{Error Analysis:}
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item Focus on misclassified cases, specifically unseen hazards.
        \item Qualitative inspection of captions for missed or misleading elements.
    \end{itemize}
\end{itemize}

\subsection*{5.2 Test cases}

Following test cases  are used to verify that the model meets its requirements :

1. Functional Testing: Ensures that each component—caption generation, keyword matching, semantic similarity, GNN-based reasoning, and classification—works as intended.

2. Performance Testing: Measures the system's efficiency and accuracy in real-time or near-real-time conditions using a separate test dataset with varied hazard scenarios.

\begin{itemize}
    \item \textbf{BLEU Score:} Assesses the overlap between generated and reference captions using n-gram precision.
    \item \textbf{ROUGE Score:} Measures recall-based overlap, useful for capturing longer phrase matches.
    \item \textbf{METEOR Score:} Considers synonymy and word order for a more comprehensive evaluation of caption relevance and fluency.
\end{itemize}

\subsection*{5.3 Testing Environment}



Hardware
\begin{itemize}
  \item GPU: NVIDIA T4 (15~GB VRAM)
  \item RAM: 12.7~GB system memory
  \item CPU: 2 vCPUs
\end{itemize}


\noindent{Software}
\begin{itemize}
  \item Python 3.10
  \item PyTorch 2.0
  \item HuggingFace Transformers
  \item PyTorch Geometric
  \item \texttt{scikit-learn}, NLTK, SentenceTransformers
\end{itemize}



\subsection*{5.4 Test data}
The test data will consist of a diverse set of real-world driving scenario images, ensuring comprehensive evaluation of the model's generalizability and robustness. This includes:

\begin{itemize}  % Requires \usepackage{enumitem}
  \item Images containing common and uncommon road hazards (e.g., potholes, debris, fallen trees, construction zones).
  
  \item Edge cases not present in the training data to evaluate out-of-label detection capabilities (e.g., a giraffe crossing the road).
  
  \item Annotated captions for ground truth comparison.
  
  \item Annotated hazard labels (low, medium, high) to validate classification accuracy.
\end{itemize}


\newpage

% Chapter 6 - Results
\section*{6. Result}
\subsection*{6.1 Caption Generation }
\vspace{1em}
\begin{figure}[!htbp]
    \centering
    \includegraphics[height=6cm,width=1\textwidth]{Figuras/image3.png}
    \caption{%
    Sample caption generation 1\\
    \textbf{\\Caption:} straight highway, daytime, good visibility, with a vehicle moderately distant ahead.\\
    \textbf{Ground Truth:} Straight highway, daytime, moderate visibility, vehicles are moderately distant ahead.\\
    \textbf{BLEU:} 0.1228 \quad \textbf{METEOR:} 0.7426 \quad \textbf{ROUGE-L:} 0.7619}
    \label{fig:fig3}
\end{figure}

\vspace{1em} % optional vertical space between figures

\begin{figure}[!htbp]
    \centering
    \includegraphics[height=6cm,width=1\textwidth]{Figuras/image4.png}
    \caption{%
    Sample caption generation 2\\
    \textbf{\\Generated:} straight road, nighttime, poor visibility, with a vehicle in the distance ahead and a ball on the left side of the road.\\
    \textbf{Ground Truth:} Straight road, nighttime, poor visibility, the road ahead is barely lit by headlights.\\
    \textbf{BLEU:} 0.1197  \quad \textbf{METEOR:} 0.4838  \quad \textbf{ROUGE-L:} 0.4000}
    \label{fig:fig4}
\end{figure}

\newpage
\subsection*{6.2 Hazard Detection }
\vspace{1em}
\begin{figure}[!htbp]
    \centering
    \includegraphics[height=6cm,width=1\textwidth]{Figuras/image5.png}
    \caption{%
    Low severity hazard detection\\
    \textbf{\\Caption:}straight rural road, daytime, good visibility, with a vehicle moderately distant ahead.\\
    \textbf{Predicted}: low  |  \textbf{Ground Truth}: low
    }  % <-- added missing closing brace here
    \label{fig:fig5}  % <-- made unique
\end{figure}

\vspace{1em} % optional vertical space between figures

\begin{figure}[!htbp]
    \centering
    \includegraphics[height=6cm,width=1\textwidth]{Figuras/image7.png}
    \caption{%
    Medium severity hazard detection\\
    \textbf{\\Caption:} straight residential street, daytime, good visibility, with a vehicle ahead at a moderate distance and a pedestrian on the left sidewalk.\\
    \textbf{Predicted}: medium |  \textbf{Ground Truth}: medium
    }  % <-- added missing closing brace here
    \label{fig:fig7}  % <-- made unique
\end{figure}


\newpage
\begin{figure}[!htbp]
    \centering
    \includegraphics[height=6cm,width=1\textwidth]{Figuras/image6.png}
    \caption{%
    High severity hazard detection\\
    \textbf{\\Caption:} straight highway, dusk, poor visibility due to heavy rain, with a vehicle ahead at a moderate distance.\\
    \textbf{Predicted}: high |  \textbf{Ground Truth}: high
    }  % <-- added missing closing brace here
    \label{fig:fig6}  % <-- made unique
\end{figure}
\subsection*{6.3 Explainable Hazard Detection }
\begin{figure}[!htbp]
    \centering
    \includegraphics[height=10cm,width=1\textwidth]{Figuras/image10.png}
    \caption{
    Attention over nodes in high severity hazard detection
    
    }  % <-- added missing closing brace here
    \label{fig:fig7}  % <-- made unique
\end{figure}

\newpage
% Chapter 7 - Evaluation
\section*{7.Evaluation}

\subsection*{7.1 Caption Generation Evaluation }
\vspace{1em}
\begin{itemize}
    \item \textbf{Corpus BLEU Score:} 0.4755
    \item \textbf{Average METEOR Score:} 0.4836
    \item \textbf{Average ROUGE-L Score:} 0.53
\end{itemize}

\subsection*{7.2 Hazard Detection Evaluation}
\begin{figure}[!htbp]
    \centering
    \includegraphics[height=10cm,width=0.9\textwidth]{Figuras/image8.png}
    \caption{Confusion Matrix}
    \label{fig:fig8}
\end{figure}


\newpage
\begin{figure}[!htbp]
    \includegraphics[height=7cm,width=0.9\textwidth]{Figuras/image9.png}
    \caption{Classification Report}
    \label{fig:fig9}
\end{figure}

\newpage


% Chapter 8 - Conclusion
\section*{8. Conclusion and Future Scope}

Autonomous Driving Systems is a vast area of research where improvements are happening rapidly with more and more new innovations coming each year. Several factors can affect the implementation of a model like dataset availability, Choice of optimization algorithm, other resources like GPU etc.

\vspace{1em}

The proposed system demonstrates the potential of combining vision-language models with semantic reasoning and graph-based learning for effective hazard detection in autonomous driving. By fine-tuning the BLIP model to generate detailed image captions and integrating semantic similarity, keyword detection, and a graph neural network trained on a custom knowledge graph, the system is capable of classifying novel hazards into low, medium, and high severity with improved interpretability and contextual understanding.

\vspace{1em}

\textbf{Future work} can focus on several key directions to enhance the system's performance and applicability. Real-time deployment is essential for practical use in dynamic driving environments, ensuring that hazard detection and caption generation occur with minimal latency. Incorporating multimodal fusion - combining visual, textual, and possibly audio or sensor data can improve the model’s ability to understand complex scenes. Continuous learning mechanisms would allow the system to adapt over time by learning from new scenarios without requiring complete retraining. Finally, training on larger and more diverse datasets would improve generalization and robustness, especially for handling rare or novel hazards.

\newpage

% Chapter 9 - References
\section*{9. References}
% redefine enumerate labels to appear as [1], [2], ...
\renewcommand\labelenumi{[\arabic{enumi}]}

\begin{enumerate}
    \item Zero-shot Hazard Identification in Autonomous Driving: A Case Study on the COOOL Benchmark by Lucas Picek et al.
    \item HazardVLM: A Video Language Model for Real-Time Hazard Description in Automated Driving Systems by Dannier Xiao et al.
    \item BLIP: Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding and Generation by Junnan Li et al.
    \item Inductive Representation Learning on Large Graphs by William Hamilton et al.
    \item DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models by Xiayu Tian et al.

\end{enumerate}

\newpage
\thispagestyle{empty}
~
\newpage

% We will manually place the "Version 1.0 approved" and "Prepared by" to control spacing precisely
\predate{} % Remove default date formatting
\postdate{} % Remove default date formatting

{\large \textbf{Appendix 1: SRS document}}\\[1.5ex]
\title{Software Requirements Specification}
\author{Vision Language Model for Out of Label Detection in Autonomous Driving}
% The \date command will be manually handled in \maketitle for precise layout
\date{} % Clear the default date.



% Custom \maketitle to replicate the image layout
\bgroup
\centering
\vspace*{1in} % Adjust vertical space from top of the page to the title
{\fontsize{36pt}{36pt}\selectfont\bfseries Software Requirements Specification}\par
  \vspace{0.5in}%
    {\fontsize{20pt}{20pt}\selectfont\bfseries for}\par
  \vspace{0.2in}%
  {\fontsize{24pt}{24pt}\selectfont\bfseries Vision Language Model Based Out of Label Hazard Detection in Autonomous Driving}\par
\vspace{0.75in} % Space after project title
\fontsize{18pt}{18pt}\selectfont
    Version 1.0 approved\par
    \vspace{0.5in} % Space after version
    Prepared by\par
    \vspace{0.2in} % Space after prepared by
    Abhiram K Aravind\par
    Ajay Suseel\par
    Kishan PK\par
    Pranav I B\par
    \vspace{0.75in}
    TKM College of Engineering\par
    \vfill % Pushes the date to the bottom
    18 February 2025\par
\egroup
\thispagestyle{empty} % No page number on the title page

\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}
\section*{Contents}
% Command to create dotted leaders inside tabularx
\newcommand{\tocentry}[2]{%
  \makebox[0pt][l]{#1}%
  \hspace*{\fill}%
  \dotfill\hspace{0.5em}#2%
}

\vspace{0.5cm}

\begin{tabularx}{\textwidth}{@{}Xr@{}}
\tocentry{\textbf{1 \quad Introduction}}{21} \\
\tocentry{\quad 1.1 \quad Purpose}{21} \\
\tocentry{\quad 1.2 \quad Document Conventions}{21} \\
\tocentry{\quad 1.3 \quad Intended Audience and Reading Suggestions}{21} \\
\tocentry{\quad 1.4 \quad Project Scope}{21} \\
\tocentry{\quad 1.5 \quad References}{21} \\[0.3em]

\tocentry{\textbf{2 \quad Overall Description}}{22} \\
\tocentry{\quad 2.1 \quad Product Perspective}{22} \\
\tocentry{\quad 2.2 \quad Operating Environment}{22} \\
\tocentry{\quad 2.3 \quad User Documentation}{23} \\[0.3em]

\tocentry{\textbf{3 \quad User Classes and Characteristics}}{23} \\
\tocentry{\quad 3.1 \quad End Users}{23} \\[0.3em]

\tocentry{\textbf{4 \quad Other Nonfunctional Requirements}}{23} \\
\tocentry{\quad 4.1 \quad Performance Requirements}{23} \\[0.3em]

\tocentry{\textbf{5 \quad Challenges/Constraints}}{23} \\[0.3em]

\tocentry{\textbf{6 \quad Functional Requirements}}{23} \\[0.3em]

\tocentry{\textbf{7 \quad Appendix A: Glossary}}{24} \\
\end{tabularx}

\newpage
\thispagestyle{empty}
~
\newpage
\setcounter{page}{1}

\section{1.Introduction}

\subsection{Purpose}
The purpose of this document is to outline the requirements for a system designed to detect out-of-label hazards in autonomous driving systems by leveraging a fine-tuned Vision-Language Model (VLM) and a robust hazard severity detection mechanism. The aim is to enhance safety and reliability in AI-driven navigation by identifying novel or rare scenarios not explicitly seen during training.

\subsection{Document Conventions}
This document is based upon:
\begin{itemize}
    \item IEEE 830-1998 - IEEE Recommended Practice for Software Requirements Specifications
    \item IEEE 830-1984 - IEEE Guide for Software Requirements Specifications
\end{itemize}

\subsection{Intended Audience and Reading Suggestions}
The document is intended for the project's panel of professors and team members, including developers and documentation writers.

\subsection{Project Scope}
This system aims to identify complex, out-of-label hazards in real-world driving scenarios that may not have been explicitly seen during training. By incorporating semantic similarity techniques and graph-based reasoning, the proposed approach enhances the generalizability and adaptability of hazard detection in autonomous driving, contributing to safer and more reliable AI-driven navigation.

\subsection{References}
\begin{itemize}
    \item "Inductive Representation Learning on Large Graphs" by William Hamilton et al.
    \item "HazardVLM: A Video Language Model for Real-Time Hazard Description in Automated Driving Systems" by Dannier Xiao et al.
    \item "BLIP: Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding and Generation" by Junnan Li et al.
    \item "DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models" by Xiayu Tian et al.
    \item "Zero-shot hazard identification in Autonomous Driving: A Case Study on the COOOL Benchmark" by Lucas Picek et al.
\end{itemize}


\section{2.Overall Description}

\subsection{Product Perspective}
This project aims to develop a comprehensive framework for detecting out-of-label hazards in autonomous driving systems by leveraging a fine-tuned BLIP model and a robust hazard detection mechanism based on generated scene captions. The BLIP model is first fine-tuned on the DHPR dataset images, annotated with hazard aware captions, to generate captions that describe potential hazards in driving scenes. These captions are then analyzed using a triple approach: keyword extraction, semantic similarity assessment against a set of predefined base cases, and a Graph Neural Network trained on image captions classified into high, low, or medium. Through this methodology, the system seeks to identify out-of-label hazards that may not have been explicitly seen during training. By incorporating GNN , the proposed approach enhances the generalizability and explainability of hazard detection in autonomous driving, contributing to safer and more reliable AI-driven navigation.

\subsection{Operating Environment}
\begin{itemize}
    \item A computer or server with sufficient processing power and memory
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item GPU: NVIDIA T4 (15 GB VRAM)
        \item RAM: 12.7 GB system memory
        \item CPU: 2 vCPUs
    \end{itemize}
    \item An operating system that supports the required software and libraries
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item Linux / Windows / Mac OS
    \end{itemize}
    \item Sufficient storage space for storing data.
    \item Software: Python 3.10, PyTorch 2.0, HuggingFace Transformers, PyTorch Geometric, scikit-learn, NLTK, SentenceTransformers.
\end{itemize}

\subsection{User Documentation}
A soft copy of the document in PDF format and a manual will be shared with customers and developers.

\section{3.User Classes and Characteristics}

\subsection{End Users}
\begin{itemize}
    \item Developers and researchers in autonomous driving systems.
    \item Entities involved in the validation and deployment of AI-driven navigation technologies.
    \item Future research initiatives focusing on hazard detection and novel scene understanding in AI.
\end{itemize}


\section{4.Other Nonfunctional Requirements}

\subsection{Performance Requirements}
\begin{itemize}
  \item \textbf{Throughput:} The system should be able to process road scene images and generate hazard classifications efficiently.
  \item \textbf{Error rate:} The deep learning models should have a low error rate in reconstruction, such as achieving an accuracy rate of 95\% or higher. For classification, this translates to aiming for high accuracy, precision, recall, and F1-score.
  \item \textbf{Caption Quality:} The system should achieve high scores on caption quality metrics such as BLEU, ROUGE, and METEOR.
\end{itemize}

\section{5.Challenges/Constraints}
\begin{itemize}
    \item Noiseless Input Image
    \item Image Views from all directions
    \item High GPU usage
    \item Reduced fine feature extraction
    \item Uncontrolled Lighting environment while taking photos
    \item Availability and diversity of labeled training data, especially for novel hazards.
    \item Generalization beyond training labels for unseen hazards.
\end{itemize}

\section{6.Functional Requirements}
\begin{itemize}
    \item Ability to fine-tune a vision-language model (BLIP) to generate context-rich natural language descriptions of driving scenes, focusing on accuracy and inclusion of hazard-related cues.
    \item Ability to extract semantic cues from generated captions using keyword matching and semantic similarity assessment.
    \item Ability to construct and utilize a knowledge graph from common patterns in training captions to infer deeper contextual understanding.
    \item Ability to classify hazards into severity ratings (e.g., low, medium, high) based on the semantic content of the generated descriptions and graph-based reasoning using a Graph Neural Network (GNN).
    \item Ability to identify out-of-label hazards that may not have been explicitly seen during training.
\end{itemize}

\section{7.Appendix A: Glossary}
\begin{itemize}
    \item \textbf{ADS:} Autonomous Driving Systems
    \item \textbf{BLIP:} Bootstrapping Language-Image Pretraining (a Vision-Language Model)
    \item \textbf{DL:} Deep Learning
    \item \textbf{GNN:} Graph Neural Network
    \item \textbf{OOD:} Out-of-Distribution
    \item \textbf{SRS:} Software Requirements Specification
    \item \textbf{VLM:} Vision-Language Model
\end{itemize}

\newpage
% Page setup

\lstdefinestyle{json}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    stringstyle=\color{black},
    commentstyle=\color{green!60!black},
    showstringspaces=false,
    tabsize=2
}

% Header and footer

\renewcommand{\headrulewidth}{0pt}

\begin{titlepage}
{\large \textbf{Appendix 2: SDD document}}\\[1.5ex]
  \vspace*{1in}  % reliable top space

  \begin{center}
    {\fontsize{36pt}{36pt}\selectfont\bfseries
      Software Design Document
    }\par
    \vspace{0.5in}
    {\fontsize{20pt}{20pt}\selectfont\bfseries
      for
    }\par
    \vspace{0.2in}
    {\fontsize{24pt}{24pt}\selectfont\bfseries
      Vision Language Model Based Out of Label Hazard Detection in Autonomous Driving
    }\par
    \vspace{0.75in}

    {\fontsize{18pt}{18pt}\selectfont
      Version 1.0 approved\par
      \vspace{0.5in}
      Prepared by\par
      \vspace{0.2in}
      Abhiram K Aravind\par
      Ajay Suseel\par
      Kishan PK\par
      Pranav I B\par
      \vspace{0.75in}
      TKM College of Engineering\par
    }

    \vfill  % push date to bottom

    {\fontsize{18pt}{18pt}\selectfont
      12 March 2025
    }\par
  \end{center}

  \thispagestyle{empty}  % no page number
\end{titlepage}
\setcounter{section}{0}   % next \section will be numbered 5
\setcounter{subsection}{0}% and its subsections start back at 1

\newpage
\thispagestyle{empty}
~
\newpage

\thispagestyle{empty}
\section*{CONTENTS}
\vspace{0.5cm}

\begin{tabularx}{\textwidth}{@{}Xr@{}}
\textbf{1 \quad Introduction} \dotfill & 2 \\
\quad 1.1 \quad Purpose \dotfill & 2 \\
\quad 1.2 \quad Scope \dotfill & 2 \\
\\
\textbf{2 \quad System Architecture} \dotfill & 3 \\
\quad 2.1 \quad System Overview \dotfill & 3 \\
\quad 2.2 \quad Architecture Diagram \dotfill & 3 \\
\\
\textbf{3 \quad Technological Stacks} \dotfill & 4 \\
\\
\textbf{4 \quad Data Design} \dotfill & 5 \\
\quad 4.1 \quad Training Data Schema \dotfill & 5 \\
\quad 4.2 \quad Hazard Log Schema \dotfill & 5 \\
\\

\textbf{5 \quad Testing Strategy} \dotfill & 7 \\
\quad 5.1 \quad Testing Scope \dotfill & 7 \\
\quad 5.2 \quad Test Cases \dotfill & 7 \\
\quad 5.3 \quad Metrics \dotfill & 7 \\
\quad 5.4 \quad Test Data \dotfill & 8 \\
\quad 5.5 \quad Test Environment \dotfill & 8 \\
\quad 5.5.1 \quad Hardware Configuration \dotfill & 8 \\
\quad 5.5.2 \quad Software Stack \dotfill & 8 \\
\end{tabularx}


\newpage
\thispagestyle{empty}
~
\newpage
\setcounter{page}{1}
\section{1.Introduction}

\subsection{ Purpose}

This document outlines the design of a vision-language model for detecting hazards in autonomous driving scenarios that are not covered by standard dataset labels. The purpose of this project is to develop a hazard detection system for autonomous driving that can identify both known and unseen hazards using vision-language understanding and graph-based reasoning.

Key goals include:

\begin{itemize}
    \item \textbf{Context-Aware Captioning}: Fine-tune the BLIP model to generate hazard-focused captions, even for out-of-distribution scenarios.
    \item \textbf{Graph-Augmented Detection}: Combine textual features, semantic similarity, and knowledge graphs to improve hazard recognition.
    \item \textbf{Explainable Predictions}: Use GNNs with attention and interpretable heuristics to provide transparent hazard classification.
\end{itemize}

\subsection{Scope}

The system is designed to detect both known and out-of-label hazards in autonomous driving scenes using a hybrid approach that combines vision-language modeling and graph-based reasoning. It:

\begin{itemize}
    \item Fine-tunes a BLIP model to generate hazard-aware captions from road scenes.
    \item Uses semantic similarity and keyword-based heuristics to identify potential risks.
    \item Employs a GraphSAGE-based GNN to classify hazards and provide structured, explainable predictions.
\end{itemize}

\newpage
\section{2.System Architecture}

\subsection{System Overview}

The system consists of the following components:

\begin{itemize}
    \item \textbf{Caption Generation}: Fine-tuned BLIP model generates descriptive captions emphasizing scene-level risk elements for an input image.
    \item \textbf{Feature Extraction \& Reasoning}: Triplets, keywords, and semantic embeddings are derived from captions.
    \item \textbf{Hazard Classification}: A GNN-based classifier (GraphSAGE) integrates extracted features and graph structure to assign hazard ratings (Low/Medium/High) with transparent reasoning through attention mechanisms.
\end{itemize}

\subsection{Architecture Diagram}

% Placeholder for architecture diagram
\begin{figure}[h]
    \centering
    \includegraphics[height=2cm,width=1\textwidth]{Figuras/image.png} % adjust width as needed
    \caption{ System Architecture Diagram}
    \label{fig1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[height=6cm,width=1\textwidth]{Figuras/image2.png} % adjust width as needed
    \caption{Architecture of GraphSAGE Classifier}
    \label{fig2}
\end{figure}
\newpage
\section{3.Technological Stack}

\begin{itemize}
    \item \textbf{Python Programming Language}: Python is a high-level, general-purpose programming language. It supports multiple programming paradigms, including structured, object-oriented, and functional programming, and is widely used in machine learning and data science projects.
    
    \item \textbf{PyTorch Framework}: PyTorch is an open-source machine learning framework based on the Torch library. It is commonly used for deep learning and natural language processing tasks due to its flexibility and strong community support.
    
    \item \textbf{Lightning Framework}: PyTorch Lightning is a lightweight PyTorch wrapper for high-performance research. It structures PyTorch code to decouple the research and engineering components, making code more readable and modular.
\end{itemize}

\textbf{Libraries Used}:

\begin{itemize}
    \item \textbf{torch}: Core PyTorch library used for tensor operations and deep learning model building.
    \item \textbf{transformers}: Hugging Face library used for implementing pre-trained models such as BLIP for vision-language tasks.
    \item \textbf{PIL (Pillow)}: Used for loading and processing image data.
    \item \textbf{torchvision}: Provides datasets, model architectures, and image transformations for computer vision.
    \item \textbf{matplotlib.pyplot}: Plotting library for generating graphs and visualizations.
    \item \textbf{tqdm}: Displays progress bars during training and evaluation loops.
    \item \textbf{nltk}: Natural Language Toolkit used for computing BLEU and METEOR evaluation scores.
    \item \textbf{rouge\_score}: Calculates ROUGE scores for evaluating textual similarity and overlap.
    \item \textbf{pytorch\_lightning.callbacks}: Provides training utilities like \textbf{ModelCheckpoint} and \textbf{EarlyStopping}.
    \item \textbf{networkx}: For creating and manipulating complex graph structures.
    \item \textbf{torch\_geometric}: PyTorch extension for deep learning on graphs and non-Euclidean data.
    \item \textbf{spacy}: NLP library used for tokenization and extracting linguistic features.
    \item \textbf{sentence-transformers}: Framework for sentence embeddings and semantic similarity tasks.
    \item \textbf{sklearn (scikit-learn)}: Used for metrics such as accuracy, precision, recall, and F1-score.
    \item \textbf{json, os, pickle}: Python standard libraries for data handling and serialization.
    \item \textbf{numpy}: Essential library for numerical computations using arrays and matrices.
\end{itemize}

\newpage
\section{4.Data Design}

\subsection{Training Data Schema}

Used for fine-tuning vision-language model.

\begin{lstlisting}[style=json, caption=Training Data Schema]
{
    "filename": "file21.jpg",
    "description": "a dog is walking down a driveway with leaves and a house at background"
}
\end{lstlisting}

\subsection{Hazard Log Schema}

Used for storing detected hazards and their risk assessment.

\begin{lstlisting}[style=json, caption=Hazard Log Schema]
{
    "filename": file121.jpg,
    "ground_truth": "low",
    "caption": straight rural road, daytime, good visibility, with a vehicle moderately distant ahead.,
    "predicted_rating": "low"
}
\end{lstlisting}


\newpage
\section{5.Testing Strategy}

Testing evaluates the system's ability to accurately generate captions and classify hazards in unseen road scenes. It ensures robustness across the full pipeline---caption generation (BLIP) and hazard classification (GNN).

\subsection{Testing Scope}

The testing scope focuses on validating the reliability and generalization of both the caption generation and hazard classification modules. Key aspects include:

\begin{itemize}
    \item \textbf{Performance Metrics}:
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item \textbf{Caption Quality}: Evaluated using BLEU, ROUGE, and METEOR scores.
        \item \textbf{Classification Accuracy}: Measured using Accuracy, Precision, Recall, and F1-score.
    \end{itemize}
    
    \item \textbf{Hyperparameter Validation}:
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item \textbf{BLIP Module}: Tested under different learning rates and with frozen/unfrozen configurations.
        \item \textbf{GNN Module}: Evaluated across multiple settings including dropout rates, number of epochs, and hidden layer dimensions.
    \end{itemize}
    
    \item \textbf{Error Analysis}:
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item Focus on misclassified samples, especially those involving previously unseen hazards.
    \end{itemize}
\end{itemize}

\subsection{Test Cases}

\begin{itemize}
    \item \textbf{Functional Testing}: Validates each module---BLIP, keyword matcher, semantic similarity, GNN reasoning, and final classifier.
    
    \item \textbf{Performance Testing}: Tests the system under real-time or near-real-time conditions using diverse hazard scenes.
    
    \item \textbf{Caption Evaluation}:
    \begin{itemize}[label=\textopenbullet,leftmargin=1.5cm]
        \item \textbf{BLEU}: N-gram precision
        \item \textbf{ROUGE}: Recall-based phrase overlap
        \item \textbf{METEOR}: Considers synonyms and word order
    \end{itemize}
\end{itemize}

\subsection{Metrics}

\begin{itemize}
    \item \textbf{Captions}: BLEU, ROUGE, METEOR
    \item \textbf{Classification}: Accuracy, Precision, Recall, F1-score
\end{itemize}

\subsection{Test Data}

\begin{itemize}
    \item Real-world  driving scenarios
    \item Hazard types: potholes, debris, construction, animals etc.
    \item Includes cases not seen during training
    \item Accurate verified labels for ground truth comparison
\end{itemize}

\subsection{Test Environment}

The testing was conducted in a controlled environment equipped with the necessary hardware and software to support GPU-accelerated model evaluation and analysis.

\subsubsection{Hardware Configuration}

\begin{itemize}
    \item \textbf{GPU}: NVIDIA T4 (≈ 15 GB VRAM)
    \item \textbf{RAM}: ≈ 12.7 GB system memory
    \item \textbf{CPU}: 2 vCPUs
\end{itemize}

\subsubsection{Software Stack}

\begin{itemize}
    \item \textbf{Python 3.10}
    \item \textbf{PyTorch 2.0}
    \item \textbf{HuggingFace Transformers}
    \item \textbf{PyTorch Geometric}
    \item \textbf{scikit-learn, NLTK, SentenceTransformers}
\end{itemize}

\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}
\begin{center}
  {\large \textbf{Appendix 3: Record of Changes}}\\[1.5ex]
  \begin{tabular}{| c | c | l | p{8cm} |}
    \hline
    \textbf{Version} & \textbf{Date} & \textbf{Author} & \textbf{Description of change} \\ 
    \hline
    1.0 & 05/03/2025 & xxx   & Dataset Annotation \\ 
    \hline
    1.0 & 15/03/2025 & xxx     & BLIP Finetuning\\ 
    \hline
    1.0 & 25/03/2025 & xxx     & Finetuned BLIP Evaluation\\ 
    \hline
    1.0 & 31/03/2025 & Ajay Suseel   & GNN Building and Training \\ 
    \hline
    1.0 & 07/04/2025 &  xxx        & Integrating BLIP and GNN into a single pipeline \\ 
    \hline
    1.0 & 09/04/2023 & xxx   & Evaluation of Hazard Detection \\ 
    \hline
    
  \end{tabular}
\end{center}
\newpage
\thispagestyle{empty}
~
\newpage
\thispagestyle{empty}
{\large \textbf{Appendix 4: Fulfillment of COs and POs}}\\[1.5ex]
\setcounter{page}{1}
\section*{FULFILLMENT OF COURSE OUTCOMES (COs)}

\begin{itemize}[leftmargin=1.5cm]
    \item[\textbf{CO1.}] Identify technically and economically feasible problems. 


    \vspace{0.2cm}


    The project addressed the social cause of empowering the visually impaired population by assisting with professional email tasks, identifying a feasible problem.
    
    \item[\textbf{CO2.}] Identify and survey the relevant literature for getting exposed to related solutions and get familiarized with software development processes. 
    
    \vspace{0.2cm}
    
    A detailed online survey was conducted to establish functional and non-functional requirements.
    
    \item[\textbf{CO3.}] Perform requirement analysis, identify design methodologies and develop adaptable \& reusable solutions of minimal complexity by using modern tools \& advanced programming techniques.
    
    \vspace{0.2cm}
    
    A detailed requirement analysis was performed and documented in an SRS Document. Flutter built-in plugins were used to enhance application functionality.
    
    \item[\textbf{CO4.}] Prepare technical report and deliver presentation.
    
    \vspace{0.2cm}
    
    A comprehensive technical report was structured and a presentation was created.
    
    \item[\textbf{CO5.}] Apply engineering and management principles to achieve the goal of the project. 
    
    \vspace{0.2cm}
    
    The Waterfall Software model was used to guide the project's development.
\end{itemize}

\vspace{1cm}
\section*{FULFILLMENT OF PROGRAM OUTCOMES (POs)}

\begin{itemize}[leftmargin=1.5cm]
    \item[\textbf{PO1.}] \textbf{Engineering Knowledge:} Demonstrated in-depth understanding of computer science principles and applied them to develop a robust and efficient system. Leveraged modern deep learning architectures and relevant frameworks to ensure optimal performance and functional correctness.

    \item[\textbf{PO2.}] \textbf{Problem Analysis:} Conducted comprehensive research and analysis to identify key requirements and effectively captured them in the system's design documentation. Analyzed and understood the challenges and complexities of the problem space to provide a tailored solution.

    \item[\textbf{PO3.}] \textbf{Design/Development of Solution:} Utilized appropriate design principles and methodologies to create an intuitive and effective solution architecture. Drew inspiration from reputable research and established practices in deep learning and computer vision to ensure a high-quality and well-structured system.

    \item[\textbf{PO4.}] \textbf{Conduct Investigation for Complex Problems:} Engaged in extensive research to identify and implement suitable deep learning and computer vision models for complex problem-solving. Explored various algorithms, techniques, and datasets to ensure accurate and reliable results.

    \item[\textbf{PO5.}] \textbf{Modern Tool Usage:} Employed cutting-edge deep learning platforms, programming languages, and frameworks (e.g., Python, PyTorch) to build the project's core functionalities. Leveraged the latest tools and libraries to enhance productivity and achieve desired model performance.

    \item[\textbf{PO6.}] \textbf{The Engineer and the Society:} Focused on addressing a relevant societal need by developing a system with potential positive impact. Emphasized considerations for fairness, transparency, and ethical implications inherent in the development and deployment of intelligent systems.

    \item[\textbf{PO7.}] \textbf{Environment and Sustainability:} Adopted coding practices that optimized model efficiency and performance, minimized computational resource consumption (e.g., GPU, memory), and adhered to industry best practices for sustainable development of AI models. Ensured efficient utilization of system resources to reduce environmental impact.

    \item[\textbf{PO8.}] \textbf{Ethics:} Maintained a high standard of ethical conduct throughout the project, particularly regarding data handling and model usage. Adhered to proper referencing and citation practices, avoiding plagiarism, and fostered a collaborative and respectful team environment, promoting professionalism and integrity.

    \item[\textbf{PO9.}] \textbf{Individual and Teamwork:} Assigned tasks and responsibilities to team members, conducted regular milestone checks, and facilitated effective collaboration. Encouraged knowledge sharing, supported team members in need, and ensured equal contribution from all team members.

    \item[\textbf{PO10.}] \textbf{Communication:} Established effective communication channels within the team, including regular meetings and online collaboration platforms. Ensured seamless coordination, shared progress updates, and addressed any project-related issues promptly.

    \item[\textbf{PO11.}] \textbf{Project Management and Finance:} Utilized a structured project management approach, considering relevant methodologies for ML/CV project development. Accounted for computational resources, data storage, and tool requirements, ensuring proper planning and resource allocation for model training and deployment.

    \item[\textbf{PO12.}] \textbf{Life-long Learning:} Embraced a growth mindset and cultivated a culture of continuous learning. Adapted to new deep learning techniques, programming paradigms, and development practices throughout the project. Encountered challenges as learning opportunities and enhanced skills in AI development and related domains.
\end{itemize}
\newpage
  \thispagestyle{empty}
  ~
 \newpage 
 \thispagestyle{empty}
{\large \textbf{Appendix 5: Project Timeline}}\\[1.5ex]
\setlength{\arrayrulewidth}{1pt}
\arrayrulecolor{black}
% 2) Keep a small gutter around every cell
\setlength{\tabcolsep}{2pt}      % 2pt on left/right of each cell
\renewcommand{\arraystretch}{1.1} % ~10% extra vertical padding

% 3) Control inner padding inside \colorbox
\setlength{\fboxsep}{2pt}        % 2pt white space between the black box and its cell border

\section*{PROJECT TIMELINE}




...

\begin{center}
\resizebox{\textwidth}{!}{  % Scales the table to full text width
\begin{tabular}{|l|c|c|c|c|}
  \hline
  \rowcolor{white}
  \textbf{Phase} 
    & \textbf{Jan} 
    & \textbf{Feb} 
    & \textbf{Mar} 
    & \textbf{Apr} \\ 
  \hline
  Literature Survey
    & \cellcolor{gray}\textcolor{white}{} 
    &                                   
    &                                   
    &                                   \\ 
  \hline
  Requirement Analysis
    &                                   
    & \cellcolor{gray}\textcolor{white}{} 
    &                                   
    &                                   \\ 
  \hline
  Architecture Design
    &                                   
    &  \cellcolor{gray}\textcolor{white}{}                                  
    & \cellcolor{gray}\textcolor{white}{} 
    &                                   \\ 
  \hline
  Implementation
    &                                   
    &                                   
    & \cellcolor{gray}\textcolor{white}{}                                   
    & \cellcolor{gray}\textcolor{white}{} \\ 
  \hline
  Testing
    &                                   
    &                                   
    &                                   
    & \cellcolor{gray}\textcolor{white}{}                                   \\ 
  \hline
\end{tabular}
}
\end{center}




\end{document}
