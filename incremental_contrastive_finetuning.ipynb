{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysuseel/MiniProject_AD/blob/main/incremental_contrastive_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, BlipConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "ABgMqIwTxPZO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GitHub Update Functions"
      ],
      "metadata": {
        "id": "U5iCRkZRxpr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pull_latest_changes(repo_path):\n",
        "    \"\"\"Pull the latest changes from GitHub.\"\"\"\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"pull\"], check=True)\n",
        "        print(\"‚úÖ Successfully pulled the latest changes.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Error pulling latest changes: {e}\")\n",
        "\n",
        "def push_to_github(repo_path, file_path, commit_message=\"Update\"):\n",
        "    \"\"\"Push updated file(s) back to GitHub.\"\"\"\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"add\", file_path], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"commit\", \"-m\", commit_message], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"push\"], check=True)\n",
        "        print(f\"üöÄ Successfully pushed {file_path} to GitHub.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Git push error: {e}\")\n"
      ],
      "metadata": {
        "id": "WRFLwpv6xnP9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Incremental Fine-Tuning Functions"
      ],
      "metadata": {
        "id": "EseH4I_4xvHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_used_files(used_files_path):\n",
        "    \"\"\"Load the set of filenames that have already been used for fine-tuning.\"\"\"\n",
        "    if os.path.exists(used_files_path):\n",
        "        with open(used_files_path, \"r\") as f:\n",
        "            used = set(json.load(f))\n",
        "        print(f\"Loaded {len(used)} used filenames from {used_files_path}.\")\n",
        "    else:\n",
        "        used = set()\n",
        "        print(\"No used files record found; starting fresh.\")\n",
        "    return used\n",
        "\n",
        "def save_used_files(used_files, used_files_path):\n",
        "    \"\"\"Save the set of used filenames to a JSON file.\"\"\"\n",
        "    with open(used_files_path, \"w\") as f:\n",
        "        json.dump(list(used_files), f)\n",
        "    print(f\"Saved {len(used_files)} used filenames to {used_files_path}.\")\n",
        "\n",
        "\n",
        "def get_new_samples(full_data, used_files):\n",
        "    \"\"\"\n",
        "    Given the full dataset (list of samples) and a set of used filenames,\n",
        "    return only the samples that are not yet used.\n",
        "    Each sample is assumed to have a \"filename\" key.\n",
        "    \"\"\"\n",
        "    new_samples = [item for item in full_data if item[\"filename\"] not in used_files]\n",
        "    print(f\"Found {len(new_samples)} new samples for fine-tuning.\")\n",
        "    return new_samples\n",
        "\n"
      ],
      "metadata": {
        "id": "9wBjYGfSxm6Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Custom dataset"
      ],
      "metadata": {
        "id": "ww7y1Yhox2M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "class FineTuningDataset(Dataset):\n",
        "    def __init__(self, data, processor, images_folder):\n",
        "        \"\"\"\n",
        "        data: list of dictionaries with keys \"filename\", \"pos_caption\", and \"neg_caption\".\n",
        "        images_folder: directory containing images.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.processor = processor\n",
        "        self.images_folder = images_folder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(self.images_folder, item[\"filename\"])\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {item['filename']}: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Tokenize positive caption using \"pos_caption\"\n",
        "        pos_encoding = self.processor(\n",
        "            text=item[\"pos_caption\"],\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "        pos_encoding = {k: v.squeeze(0) for k, v in pos_encoding.items()}\n",
        "\n",
        "        # Tokenize negative caption using \"neg_caption\"\n",
        "        neg_encoding = self.processor(\n",
        "            text=item[\"neg_caption\"],\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "        neg_encoding = {k: v.squeeze(0) for k, v in neg_encoding.items()}\n",
        "\n",
        "        # Set labels for contrastive loss\n",
        "        pos_encoding[\"pos_labels\"] = pos_encoding[\"input_ids\"]\n",
        "        pos_encoding[\"neg_labels\"] = neg_encoding[\"input_ids\"]\n",
        "\n",
        "        return pos_encoding\n",
        "\n",
        "def create_dataloader(data, processor, images_folder, batch_size=2):\n",
        "    dataset = FineTuningDataset(data, processor, images_folder)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        if not batch:\n",
        "            return None\n",
        "        keys = batch[0].keys()\n",
        "        return {key: torch.stack([b[key] for b in batch]) for key in keys}\n",
        "\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "RN-a-zQDxyCl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contrastive loss function"
      ],
      "metadata": {
        "id": "YulVAS-fx-EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(image_embeds, pos_text_embeds, neg_text_embeds, margin=1.0):\n",
        "    pos_sim = torch.cosine_similarity(image_embeds, pos_text_embeds, dim=-1)\n",
        "    neg_sim = torch.cosine_similarity(image_embeds, neg_text_embeds, dim=-1)\n",
        "    loss = torch.relu(margin - pos_sim + neg_sim).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "IXW03gbIx6KB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading BLIP2"
      ],
      "metadata": {
        "id": "6Rbrh45wyFkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_blip2_model_with_lora(base_model_name, model_save_path):\n",
        "    \"\"\"\n",
        "    Load a previously fine-tuned model if it exists; otherwise, load the base model.\n",
        "    Apply LoRA to the vision encoder's QKV modules.\n",
        "    \"\"\"\n",
        "    from transformers import BlipProcessor, BlipForConditionalGeneration, BlipConfig\n",
        "    if os.path.exists(model_save_path):\n",
        "        print(\"Loading previously fine-tuned model...\")\n",
        "        processor = BlipProcessor.from_pretrained(model_save_path, ignore_mismatched_sizes=True)\n",
        "        model = BlipForConditionalGeneration.from_pretrained(model_save_path, ignore_mismatched_sizes=True)\n",
        "    else:\n",
        "        print(\"No previously fine-tuned model found; loading base model...\")\n",
        "        config = BlipConfig.from_pretrained(base_model_name)\n",
        "        processor = BlipProcessor.from_pretrained(base_model_name, ignore_mismatched_sizes=True)\n",
        "        model = BlipForConditionalGeneration.from_pretrained(base_model_name, config=config, ignore_mismatched_sizes=True)\n",
        "        # Apply LoRA\n",
        "        target_modules = [f\"vision_model.encoder.layers.{i}.self_attn.qkv\" for i in range(12)]\n",
        "        lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, target_modules=target_modules)\n",
        "        model = get_peft_model(model, lora_config)\n",
        "    return model, processor\n"
      ],
      "metadata": {
        "id": "nfvudymNyCDj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetuning..."
      ],
      "metadata": {
        "id": "1l0gt4R5yMqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def incremental_finetuning(new_data_json, new_images_folder, used_files_path,\n",
        "                           model_save_path, base_model_name, num_epochs=3,\n",
        "                           learning_rate=5e-5, batch_size=2, override_incremental=False):\n",
        "    # Load full dataset from your JSON file (e.g., \"contrastive_captions.json\")\n",
        "    with open(CAPTIONS_PATH, \"r\") as f:\n",
        "        full_data = json.load(f)\n",
        "\n",
        "    # Load previously used filenames\n",
        "    used_files = load_used_files(USED_FILES_PATH)\n",
        "\n",
        "    # Filter new samples (only those not in used_files)\n",
        "    new_samples = get_new_samples(full_data, used_files)\n",
        "    if not new_samples:\n",
        "        print(\"No new samples to fine-tune on. Exiting incremental fine-tuning.\")\n",
        "        exit()\n",
        "\n",
        "    # Load the BLIP processor before creating the DataLoader\n",
        "    from transformers import BlipProcessor\n",
        "    processor = BlipProcessor.from_pretrained(base_model_name, ignore_mismatched_sizes=True)\n",
        "\n",
        "    # (Pass new_samples to your DataLoader creation function.)\n",
        "    dataloader = create_dataloader(new_samples, processor, NEW_IMAGES_FOLDER, batch_size=2)\n",
        "\n",
        "    # Pull latest changes from GitHub (optional; requires authentication configured)\n",
        "    pull_latest_changes(GIT_LOCAL_PATH)\n",
        "\n",
        "    # Load new fine-tuning data from JSON file\n",
        "    try:\n",
        "        with open(new_data_json, \"r\") as f:\n",
        "            new_data = json.load(f)\n",
        "        print(f\"Loaded {len(new_data)} new fine-tuning samples from {new_data_json}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading new fine-tuning data: {e}\")\n",
        "        return\n",
        "\n",
        "    # Load used files list and filter new samples\n",
        "    if override_incremental:\n",
        "        used_files = set()  # Reset used_files to start from scratch\n",
        "        print(\"‚ö†Ô∏è Override incremental: Starting fine-tuning from the beginning.\")\n",
        "    else:\n",
        "        used_files = load_used_files(used_files_path)\n",
        "    new_samples = get_new_samples(new_data, used_files)\n",
        "    if not new_samples:\n",
        "        print(\"No new samples to fine-tune on. Exiting incremental fine-tuning.\")\n",
        "        return\n",
        "\n",
        "    # Load the previously fine-tuned model or base model with LoRA applied\n",
        "    model, processor = load_blip2_model_with_lora(base_model_name, model_save_path)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Create DataLoader for new samples\n",
        "    dataloader = create_dataloader(new_samples, processor, new_images_folder, batch_size=batch_size)\n",
        "    if dataloader is None:\n",
        "        print(\"No valid data samples found. Exiting incremental fine-tuning.\")\n",
        "        return\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    print(f\"üöÄ Starting incremental fine-tuning on {device} for {num_epochs} epochs...\")\n",
        "    for epoch in range(num_epochs):\n",
        "            epoch_loss = 0.0\n",
        "            for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "                optimizer.zero_grad()\n",
        "                pixel_values = batch[\"pixel_values\"].to(device)\n",
        "                # Change: Convert input_ids to float before setting requires_grad\n",
        "                pos_input_ids = batch[\"pos_labels\"].to(device).float()  # Convert to float\n",
        "                neg_input_ids = batch[\"neg_labels\"].to(device).float()  # Convert to float\n",
        "                # You might not need requires_grad on input_ids at all if they're not being trained\n",
        "                # pos_input_ids.requires_grad_(True)\n",
        "                # neg_input_ids.requires_grad_(True)\n",
        "                image_embeds = model.vision_model(pixel_values).last_hidden_state.mean(dim=1)\n",
        "                # Removed torch.no_grad() block to enable gradient calculation\n",
        "                # Change: Pass input_ids as long\n",
        "                pos_outputs = model.text_decoder(input_ids=pos_input_ids.long(), output_hidden_states=True)\n",
        "                neg_outputs = model.text_decoder(input_ids=neg_input_ids.long(), output_hidden_states=True)\n",
        "                pos_text_embeds = pos_outputs.hidden_states[-1].mean(dim=1)\n",
        "                neg_text_embeds = neg_outputs.hidden_states[-1].mean(dim=1)\n",
        "                loss = contrastive_loss(image_embeds, pos_text_embeds, neg_text_embeds)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "            print(f\"‚úÖ Epoch {epoch+1} completed | Average Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n",
        "\n",
        "    # Save updated model and processor\n",
        "    model.save_pretrained(model_save_path)\n",
        "    processor.save_pretrained(model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # Update used files list with new samples\n",
        "    # After fine-tuning completes, update used_files with filenames from new_samples\n",
        "    new_used_files = {item[\"filename\"] for item in new_samples}\n",
        "    used_files.update(new_used_files)\n",
        "    save_used_files(used_files, USED_FILES_PATH)\n",
        "\n",
        "\n",
        "\n",
        "    return model, processor"
      ],
      "metadata": {
        "id": "GO0xdgRnyHoV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Github push and pull"
      ],
      "metadata": {
        "id": "1ye-e8IVySYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pull_latest_changes(repo_path):\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"pull\"], check=True)\n",
        "        print(\"‚úÖ Pulled latest changes from GitHub.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Error pulling latest changes: {e}\")\n",
        "\n",
        "def push_to_github(repo_path, file_path, commit_message=\"Updated\"):\n",
        "    try:\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"add\", file_path], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"commit\", \"-m\", commit_message], check=True)\n",
        "        subprocess.run([\"git\", \"-C\", repo_path, \"push\"], check=True)\n",
        "        print(f\"üöÄ Pushed {file_path} to GitHub.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ö†Ô∏è Error pushing {file_path} to GitHub: {e}\")"
      ],
      "metadata": {
        "id": "Y5DwLcuoyPqB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BASE_MODEL_NAME = \"Salesforce/blip2-flan-t5-xl\"  # Base model identifier\n",
        "MODEL_SAVE_PATH = \"./models/finetuned_blip2\"       # Directory where fine-tuned model is saved\n",
        "\n",
        "# GitHub repository settings (adjust these paths accordingly)\n",
        "GIT_REPO_URL = \"https://github.com/ajaysuseel/MiniProject_AD.git\"\n",
        "GIT_LOCAL_PATH = \"/content/MiniProject_AD\"  # Local path where repo is cloned\n",
        "\n",
        "CAPTIONS_PATH = os.path.join(GIT_LOCAL_PATH, \"pranav\", \"contrastive_captions.json\")\n",
        "USED_FILES_PATH = os.path.join(GIT_LOCAL_PATH, \"pranav\", \"used_files.json\")\n",
        "NEW_IMAGES_FOLDER = os.path.join(GIT_LOCAL_PATH, \"pranav\", \"images\")  # All images are here"
      ],
      "metadata": {
        "id": "qN6XIR9GxTBZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"ajaysuseel673@gmail.com\"\n",
        "!git config --global user.name \"ajaysuseel\""
      ],
      "metadata": {
        "id": "_HOzl2JF20vZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Ensure your repository is cloned in Colab:\n",
        "    if not os.path.exists(GIT_LOCAL_PATH):\n",
        "        subprocess.run([\"git\", \"clone\", GIT_REPO_URL, GIT_LOCAL_PATH], check=True)\n",
        "    else:\n",
        "        pull_latest_changes(GIT_LOCAL_PATH)\n",
        "\n",
        "    # Set the paths for the new fine-tuning data (e.g., images from file62 onward)\n",
        "    NEW_DATA_JSON = os.path.join(GIT_LOCAL_PATH, \"ajay\", \"contrastive_captions.json\")\n",
        "    # (You can update NEW_DATA_JSON to point to a different file if you split your data)\n",
        "    # For example, if you have a separate JSON for new images, use that path.\n",
        "    # Here we assume the JSON contains all samples; our filtering will exclude already used ones.\n",
        "\n",
        "    # Perform incremental fine-tuning\n",
        "    model, processor = incremental_finetuning(\n",
        "        new_data_json=NEW_DATA_JSON,\n",
        "        new_images_folder=NEW_IMAGES_FOLDER,\n",
        "        used_files_path=USED_FILES_PATH,\n",
        "        model_save_path=MODEL_SAVE_PATH,\n",
        "        base_model_name=BASE_MODEL_NAME,\n",
        "        num_epochs=3,\n",
        "        learning_rate=5e-5,\n",
        "        batch_size=2,\n",
        "        override_incremental=True\n",
        "    )"
      ],
      "metadata": {
        "id": "FrafIlZyyVbV",
        "outputId": "bac4cb9c-6351-487a-dbca-79b2b8efe5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "caa56ac83dcf403c9e754a8ff80dee06",
            "7bb35644270147a09087625ab911bcb0",
            "18258e506dfe425ca52fa1e596dee217",
            "6771fd5f112042d5974bdbe95bb90faa",
            "606e965f0c554ef2a7c3ffb2f646e098",
            "bbbea40995574fad8bb52c3c6cc3cc4a",
            "b1d6102b78b3480584bff28f2b585213",
            "adb451271bc44fa6823833cfad43295d",
            "9ede42cbe4af4e9b917be40fc400d350",
            "0d836f7ec396412cb9ed0c09223433e3",
            "f1aa8252891c446599388f0db95eddd0"
          ]
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Pulled latest changes from GitHub.\n",
            "No used files record found; starting fresh.\n",
            "Found 10 new samples for fine-tuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
            "The class this function is called from is 'BertTokenizerFast'.\n",
            "Some kwargs in processor config are unused and will not have any effect: num_query_tokens. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Pulled latest changes from GitHub.\n",
            "Loaded 10 new fine-tuning samples from /content/MiniProject_AD/ajay/contrastive_captions.json.\n",
            "‚ö†Ô∏è Override incremental: Starting fine-tuning from the beginning.\n",
            "Found 10 new samples for fine-tuning.\n",
            "Loading previously fine-tuned model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type blip-2 to instantiate a model of type blip. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caa56ac83dcf403c9e754a8ff80dee06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BlipForConditionalGeneration were not initialized from the model checkpoint at Salesforce/blip2-flan-t5-xl and are newly initialized: ['text_decoder.bert.embeddings.LayerNorm.bias', 'text_decoder.bert.embeddings.LayerNorm.weight', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.0.output.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_decoder.bert.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.self.key.bias', 'text_decoder.bert.encoder.layer.10.attention.self.key.weight', 'text_decoder.bert.encoder.layer.10.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.weight', 'text_decoder.bert.encoder.layer.10.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.output.dense.bias', 'text_decoder.bert.encoder.layer.10.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.self.key.bias', 'text_decoder.bert.encoder.layer.11.attention.self.key.weight', 'text_decoder.bert.encoder.layer.11.attention.self.query.bias', 'text_decoder.bert.encoder.layer.11.attention.self.query.weight', 'text_decoder.bert.encoder.layer.11.attention.self.value.bias', 'text_decoder.bert.encoder.layer.11.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.output.dense.bias', 'text_decoder.bert.encoder.layer.11.output.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.bias', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'text_decoder.bert.encoder.layer.2.attention.self.value.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.2.output.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.self.key.bias', 'text_decoder.bert.encoder.layer.3.attention.self.key.weight', 'text_decoder.bert.encoder.layer.3.attention.self.query.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.weight', 'text_decoder.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder.bert.encoder.layer.3.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.output.dense.bias', 'text_decoder.bert.encoder.layer.3.output.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.bias', 'text_decoder.bert.encoder.layer.4.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.bias', 'text_decoder.bert.encoder.layer.4.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.bias', 'text_decoder.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.output.dense.bias', 'text_decoder.bert.encoder.layer.4.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.query.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.bias', 'text_decoder.bert.encoder.layer.5.attention.self.value.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.output.dense.bias', 'text_decoder.bert.encoder.layer.5.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.self.key.bias', 'text_decoder.bert.encoder.layer.6.attention.self.key.weight', 'text_decoder.bert.encoder.layer.6.attention.self.query.bias', 'text_decoder.bert.encoder.layer.6.attention.self.query.weight', 'text_decoder.bert.encoder.layer.6.attention.self.value.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.output.dense.bias', 'text_decoder.bert.encoder.layer.6.output.dense.weight', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.attention.self.key.bias', 'text_decoder.bert.encoder.layer.7.attention.self.key.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.bias', 'text_decoder.bert.encoder.layer.7.attention.self.query.weight', 'text_decoder.bert.encoder.layer.7.attention.self.value.bias', 'text_decoder.bert.encoder.layer.7.attention.self.value.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.output.dense.bias', 'text_decoder.bert.encoder.layer.7.output.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.self.key.weight', 'text_decoder.bert.encoder.layer.8.attention.self.query.bias', 'text_decoder.bert.encoder.layer.8.attention.self.query.weight', 'text_decoder.bert.encoder.layer.8.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.output.dense.bias', 'text_decoder.bert.encoder.layer.8.output.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.self.key.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.weight', 'text_decoder.bert.encoder.layer.9.attention.self.query.bias', 'text_decoder.bert.encoder.layer.9.attention.self.query.weight', 'text_decoder.bert.encoder.layer.9.attention.self.value.bias', 'text_decoder.bert.encoder.layer.9.attention.self.value.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.output.dense.bias', 'text_decoder.bert.encoder.layer.9.output.dense.weight', 'text_decoder.cls.predictions.bias', 'text_decoder.cls.predictions.decoder.bias', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BlipForConditionalGeneration were not initialized from the model checkpoint at Salesforce/blip2-flan-t5-xl and are newly initialized because the shapes did not match:\n",
            "- vision_model.embeddings.class_embedding: found shape torch.Size([1, 1, 1408]) in the checkpoint and torch.Size([1, 1, 768]) in the model instantiated\n",
            "- vision_model.embeddings.patch_embedding.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.embeddings.patch_embedding.weight: found shape torch.Size([1408, 3, 14, 14]) in the checkpoint and torch.Size([768, 3, 16, 16]) in the model instantiated\n",
            "- vision_model.embeddings.position_embedding: found shape torch.Size([1, 257, 1408]) in the checkpoint and torch.Size([1, 577, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.0.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.1.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.10.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.11.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.2.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.3.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.4.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.5.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.6.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.7.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.8.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.layer_norm1.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.layer_norm1.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.layer_norm2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.layer_norm2.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.mlp.fc1.bias: found shape torch.Size([6144]) in the checkpoint and torch.Size([3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.mlp.fc1.weight: found shape torch.Size([6144, 1408]) in the checkpoint and torch.Size([3072, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.mlp.fc2.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.mlp.fc2.weight: found shape torch.Size([1408, 6144]) in the checkpoint and torch.Size([768, 3072]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.self_attn.projection.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.self_attn.projection.weight: found shape torch.Size([1408, 1408]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.self_attn.qkv.bias: found shape torch.Size([4224]) in the checkpoint and torch.Size([2304]) in the model instantiated\n",
            "- vision_model.encoder.layers.9.self_attn.qkv.weight: found shape torch.Size([4224, 1408]) in the checkpoint and torch.Size([2304, 768]) in the model instantiated\n",
            "- vision_model.post_layernorm.bias: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "- vision_model.post_layernorm.weight: found shape torch.Size([1408]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting incremental fine-tuning on cuda for 3 epochs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image file49.jpg: [Errno 2] No such file or directory: '/content/MiniProject_AD/pranav/images/file49.jpg'\n",
            "Error loading image file48.jpg: [Errno 2] No such file or directory: '/content/MiniProject_AD/pranav/images/file48.jpg'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-69f537f49b3d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Perform incremental fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     model, processor = incremental_finetuning(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnew_data_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNEW_DATA_JSON\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnew_images_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNEW_IMAGES_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4ee6eec487eb>\u001b[0m in \u001b[0;36mincremental_finetuning\u001b[0;34m(new_data_json, new_images_folder, used_files_path, model_save_path, base_model_name, num_epochs, learning_rate, batch_size, override_incremental)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# Change: Convert input_ids to float before setting requires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mpos_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "push_to_github(GIT_LOCAL_PATH, used_files_path, \"Updated used files after incremental fine-tuning\")\n",
        "push_to_github(GIT_LOCAL_PATH, model_save_path, \"Saved updated fine-tuned BLIP-2 model\")\n"
      ],
      "metadata": {
        "id": "38LXgai52rCs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "caa56ac83dcf403c9e754a8ff80dee06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bb35644270147a09087625ab911bcb0",
              "IPY_MODEL_18258e506dfe425ca52fa1e596dee217",
              "IPY_MODEL_6771fd5f112042d5974bdbe95bb90faa"
            ],
            "layout": "IPY_MODEL_606e965f0c554ef2a7c3ffb2f646e098"
          }
        },
        "7bb35644270147a09087625ab911bcb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbbea40995574fad8bb52c3c6cc3cc4a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b1d6102b78b3480584bff28f2b585213",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "18258e506dfe425ca52fa1e596dee217": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adb451271bc44fa6823833cfad43295d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ede42cbe4af4e9b917be40fc400d350",
            "value": 2
          }
        },
        "6771fd5f112042d5974bdbe95bb90faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d836f7ec396412cb9ed0c09223433e3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f1aa8252891c446599388f0db95eddd0",
            "value": "‚Äá2/2‚Äá[00:00&lt;00:00,‚Äá‚Äá2.87it/s]"
          }
        },
        "606e965f0c554ef2a7c3ffb2f646e098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbbea40995574fad8bb52c3c6cc3cc4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d6102b78b3480584bff28f2b585213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adb451271bc44fa6823833cfad43295d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ede42cbe4af4e9b917be40fc400d350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d836f7ec396412cb9ed0c09223433e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1aa8252891c446599388f0db95eddd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}