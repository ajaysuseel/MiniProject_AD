{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysuseel/MiniProject_AD/blob/main/Project_app_with_explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUbc9xTBLOZt",
        "outputId": "30453ddf-96bd-498c-8ac2-5826707919a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv \\\n",
        "    -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "\n",
        "!pip install -q torch-geometric spacy pytorch-lightning sentence-transformers transformers\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S17m5yMvLMt8",
        "outputId": "0a0303f7-b345-415f-b295-c190ba88217f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import networkx as nx\n",
        "import torch.nn as nn\n",
        "import spacy\n",
        "import pytorch_lightning as pl\n",
        "from PIL import Image\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import pickle\n",
        "\n",
        "DEVICE            = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "DRIVE_ROOT        = '/content/drive'\n",
        "BLIP_CKPT_PATH    = os.path.join(DRIVE_ROOT, 'MyDrive/gemini_models/blip_checkpoints_17_04/blip-epoch=02-val_loss=0.0536.ckpt')\n",
        "GNN_WEIGHTS_PATH  = os.path.join(DRIVE_ROOT, 'MyDrive/gemini_models/kg_models2/checkpoints/best_model.pth')\n",
        "KG_PATH           = os.path.join(DRIVE_ROOT, 'MyDrive/gemini_models/kg_models2/kg_graph_aug.gpickle')\n",
        "\n",
        "class BlipLightning(pl.LightningModule):\n",
        "    def __init__(self, model_name=\"Salesforce/blip-image-captioning-base\", learning_rate=5e-5, freeze_vision=True, freeze_layers=6):\n",
        "        super().__init__()\n",
        "        self.model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.processor = BlipProcessor.from_pretrained(model_name, use_fast=True)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        if freeze_vision:\n",
        "            # Freeze vision embedding layers\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if \"vision_model.embeddings\" in name:\n",
        "                    param.requires_grad = False\n",
        "            # Freeze early vision encoder layers\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if \"vision_model.encoder.layers\" in name:\n",
        "                    parts = name.split(\".\")\n",
        "                    try:\n",
        "                        layer_index = int(parts[3])\n",
        "                    except (IndexError, ValueError):\n",
        "                        layer_index = None\n",
        "                    if layer_index is not None and layer_index < freeze_layers:\n",
        "                        param.requires_grad = False\n",
        "\n",
        "    def forward(self, pixel_values, input_ids=None, attention_mask=None, labels=None):\n",
        "        return self.model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "def load_blip(checkpoint_path):\n",
        "    lit = BlipLightning.load_from_checkpoint(checkpoint_path)\n",
        "    lit.to(DEVICE).eval()\n",
        "    return lit, lit.processor\n",
        "\n",
        "blip, blip_processor = load_blip(BLIP_CKPT_PATH)\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Load KG & Prepare Graph\n",
        "# ----------------------------\n",
        "with open(KG_PATH, \"rb\") as f:\n",
        "    G = pickle.load(f)\n",
        "\n",
        "node_to_idx = {n:i for i,n in enumerate(G.nodes())}\n",
        "deg = dict(G.degree()); max_deg = max(deg.values()) or 1\n",
        "edge_index = torch.tensor([[node_to_idx[u] for u,v in G.edges()],\n",
        "                           [node_to_idx[v] for u,v in G.edges()]], dtype=torch.long)\n",
        "x_feat = torch.tensor([[deg[n]/max_deg] for n in G.nodes()], dtype=torch.float)\n",
        "data_graph = Data(x=x_feat, edge_index=edge_index).to(DEVICE)\n",
        "\n",
        "# ----------------------------\n",
        "# 5. NLP & Embedding Setup\n",
        "# ----------------------------\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "hazard_keywords = {\n",
        "    \"low\": [\n",
        "        (\"clear road\", 1.0), (\"dry road\", 1.0), (\"daytime\", 1.0), (\"straight road\", 1.0),\n",
        "        (\"good visibility\", 1.0), (\"no traffic\", 1.0), (\"open road\", 1.0), (\"well-lit\", 1.0),\n",
        "        (\"sunny\", 1.0), (\"flat terrain\", 1.0), (\"light traffic\", 1.0), (\"wide road\", 1.0)\n",
        "    ],\n",
        "    \"medium\": [\n",
        "        (\"moderate visibility\", 1.5), (\"residential area\", 1.5), (\"curved road\", 1.5),\n",
        "        (\"slightly wet\", 1.5), (\"light rain\", 1.5), (\"children nearby\", 1.5),\n",
        "        (\"cyclist\", 1.5), (\"school zone\", 1.5), (\"urban traffic\", 1.5),\n",
        "        (\"construction zone\", 1.6), (\"intersections\", 1.6), (\"speed bumps\", 1.5)\n",
        "    ],\n",
        "    \"high\": [\n",
        "        (\"poor visibility\", 2.0), (\"fog\", 2.0), (\"heavy rain\", 2.1), (\"pedestrian ahead\", 2.2),\n",
        "        (\"jaywalking\", 2.2), (\"nighttime\", 2.0), (\"icy road\", 2.3), (\"sharp turn\", 2.1),\n",
        "        (\"blind spot\", 2.1), (\"narrow lane\", 2.0), (\"heavy traffic\", 2.0),\n",
        "        (\"obstruction\", 2.1), (\"collision\", 2.5), (\"emergency vehicle\", 2.2), (\"road closed\", 2.3)\n",
        "    ]\n",
        "}\n",
        "\n",
        "def extract_triplets(text):\n",
        "    doc = nlp(text.lower())\n",
        "    triples = []\n",
        "    for t in doc:\n",
        "        if t.dep_ in (\"amod\",\"acomp\") and t.head.pos_==\"NOUN\":\n",
        "            triples.append((t.head.lemma_, t.lemma_))\n",
        "        elif t.dep_==\"attr\" and t.head.pos_==\"NOUN\":\n",
        "            triples.append((t.head.lemma_, t.lemma_))\n",
        "        elif t.dep_==\"nsubj\" and t.head.pos_ in (\"VERB\",\"AUX\"):\n",
        "            triples.append((t.text, t.head.lemma_))\n",
        "    return triples\n",
        "\n",
        "def keyword_hazard_score(text):\n",
        "    txt = text.lower()\n",
        "    return torch.tensor([\n",
        "        sum(txt.count(kw)*w for kw,w in hazard_keywords[lvl])\n",
        "        for lvl in (\"low\",\"medium\",\"high\")\n",
        "    ], dtype=torch.float)\n",
        "\n",
        "def graph_context_score(text):\n",
        "    nodes = {u for u,_ in extract_triplets(text)} | {v for _,v in extract_triplets(text)}\n",
        "    vals = [deg[n]/max_deg for n in nodes if n in deg]\n",
        "    m = float('nan') if not vals else sum(vals)/len(vals)\n",
        "    return torch.tensor([m]*3, dtype=torch.float)\n",
        "\n",
        "def semantic_score(text):\n",
        "    emb = embedder.encode(text, convert_to_tensor=True)\n",
        "    sims = []\n",
        "    for lvl in (\"low\",\"medium\",\"high\"):\n",
        "        kws = [kw for kw,_ in hazard_keywords[lvl]]\n",
        "        kws_emb = embedder.encode(kws, convert_to_tensor=True)\n",
        "        sims.append(util.cos_sim(emb, kws_emb).max().item())\n",
        "    total = sum(sims) or 1.0\n",
        "    return torch.tensor([s/total for s in sims], dtype=torch.float)\n",
        "\n",
        "def compute_features(text):\n",
        "    return torch.cat([\n",
        "        keyword_hazard_score(text),\n",
        "        graph_context_score(text),\n",
        "        semantic_score(text)\n",
        "    ]).unsqueeze(0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784,
          "referenced_widgets": [
            "1b8de767a0464eae91e3e5f913ad0aea",
            "14f2d72b946c4579b7a81054ffa1029c",
            "927646bcea9b4740b278f1c3212367c6",
            "53fab49025fe44a6a9b7f17724a55215",
            "153bee12289c4adc997f38bad6885902",
            "0c9be4f472e74927ac23afc24acc4d7a",
            "e95c7dc19613411ca3d24429577fc27a",
            "02499a65f1a94eaea95f19e90a5f6733",
            "9a98dbf62c4848198abfa99e0fd3f6c0",
            "af1e58e2620f4255adcf2a2c46dc83aa",
            "88355f6abe8f4f0fad988b3b83f6d798",
            "fc0b3555e82b444da632955d57f2ae84",
            "103db9d20dd34e75abe21c1bbade42e1",
            "585c205d5c864fa7aa85ac78d26d8af4",
            "fd9223018e3b4f628401c3a7244f3ea6",
            "a9def5c25cd34e3fb7db8120f130bf57",
            "ba5a44836167407db5058da06462fff6",
            "0e744b1c0cd142e88640a6d535468edb",
            "538a3da8759b4739b7538da4776db258",
            "a06676f7eed5477394d200bd76f81adf",
            "6e30ae05757a43e1a81fd1b5c88b3819",
            "bbd3996e6cc847259c9f0b594f0524df",
            "2b3dd170a42c49ac9ae2466a9914f9bb",
            "b381be096edd47ab93a584595fbbb611",
            "dee91231fed64a0ab4397272268e9a4f",
            "cc93f31cecdf45d7ab6df75aaf5726ae",
            "a81baed9809a45ec8a80a8f979b35bcd",
            "2ac98b817eb44d4698a4a5fcf1fe3289",
            "80069286b1c14c8aacc915f835779f44",
            "d3465663f7384778a22303b33ab8d094",
            "7bb3e75c3a364d84a05ca73b6e92df52",
            "533b5716d59e46feb4e288c7a4a9a9a3",
            "42d0491c67974f679db76c822ef4bb2d",
            "716f2f8e25784cf1850145427c63f011",
            "de1a97c2055c4a029dfb4e4987ec32ec",
            "df9c33aa703447f588d3e9199aee5c47",
            "bbadbd055a4b4b52963eb6be1cbdc62e",
            "597e4fc2bb884fd5a98d8b10157c6152",
            "61cc64b6356e424bb4fab946d8fe7481",
            "83f5d06b53c5488287b54a5811ab9398",
            "45e79e7fa8c940b5944524bd1e415c8e",
            "7f65b5f76bbf413c9d55e836d346f6c0",
            "18156a8c2db14a63b0917e3389efae52",
            "7a6b68454a434438b466fe7e8ab3e364",
            "dd28a781d9e9468ab8eb043b65faaa83",
            "2f9566561b0048c9a2d86a1f965664b2",
            "e370d3ecc997454f86d427902c3dea5c",
            "bf07ea72872b45adb5318ec430df912d",
            "b9a6a9d013f04262bb917395b4efc689",
            "066c65bdff374de195a422bd0e2f5b24",
            "41b7dbaf19f74f8aa43f3b79d8c2a7ee",
            "fe2441aadfba46729ca94a8651e09321",
            "b2c72483757f44fea039b2c36dbe6bb7",
            "9958e7b85f4343cf98bd8136a4b228f4",
            "c44885f723f84c9babc6da963ad000fd",
            "3ef2b126950749d1824fd2370090a8f5",
            "1c3dbc47b7704eb4b44099c1fc51a7b5",
            "35fb70d89d7643468c13c67441685872",
            "7253581b7c484cd38cf72220e76dac35",
            "0a3c356f7ddd46d393f12d8c4b25ce8c",
            "aab8afbdc2264b889431af5a12dd3bd1",
            "23f855031d784cdf91e69c76e002f0b8",
            "ca24da6942aa4317aa816b85c83283db",
            "4cd4ddd2b0894d2496c070daf8d57192",
            "0e464c8d76f245e3b16a0027c64151e8",
            "7fcd96ee6f3e4ea5ab1bf9bb21668b2f",
            "074ba694002b4695aade1837db969e86",
            "4a94a722c2be4432b111843d7eda5f93",
            "a6249a12f28548459d3b61d9468fa9f1",
            "503aa50e25d649dabd242bd928b3683b",
            "af5dd8c9d399479a8e0e20fa4ed65ae5",
            "7056555abb674820b9a8dd2400e2ac22",
            "e22bc01f5e7f4a598cf1dc5df197df1c",
            "18af2a7d5efd427e806d1efa762a89fa",
            "2f0f83191d314b27a84288a55b0c40c5",
            "481c1220651d4784be16084e3151e6c7",
            "cce9eff4b20c4d4aa240f8078c1a5abe",
            "517b03ec9c1e4b62a8ddc6e85c15de4d",
            "6fbe85815c044a0092e40901443c8ea0",
            "041a5e939bf742218c5f3c2155caf7a4",
            "a94a04fb4a6b47198d137354df713a8c",
            "ee3b9187200049728bf3a0c5512c29c5",
            "9646f32440c7417a93d9aa2d5632a454",
            "45b03216a23942a0942145ebab72b1e4",
            "4b524c354bb4404896daab2b09b35ac3",
            "08a4dce1fe9045189a5e3b56b58c5850",
            "70261f1ad83648408ecfb533df96b689",
            "116d20e680884ac189a6d79e2acc2d58",
            "9f4d1b1b37f4486cb7bfa0d158c1439d",
            "2036ac80bef04df787eb5fab5971172c",
            "c18375703b4d4a50b0eba7c2f9217c80",
            "9f7f57901a3242a2b5d01c8b56572a8d",
            "10a0803df136448199589aaf75c9e1e0",
            "d01593dd1975406bb73257cc2c67ba92",
            "2aad1b253440405087be65de46977380",
            "9230b358a0a3434799684aaf72e92034",
            "fa1d8ee9b2674ea2a13f3e297ad849e9",
            "b84de497679b49bcb7edb4c3c55436ce",
            "43a8a966b9e041b9a9a75c824d9c1eb3",
            "74f8701ca12c4125acb9e28146899e88",
            "6f8133aa35e744e38484a11a0c07e900",
            "739a5ade11d749c6885299cddfbc5d4d",
            "9e85c39059a241eb9469c6efbfd1e07f",
            "ad43e65aacac416a82a2eb687e9b7505",
            "0e47891aff8f4fc7b565d0d275c86750",
            "6c3cb1a075244fb38a22141fe7cb0bb4",
            "3f6a7fb4a3d648c5ba36e966ee8763cd",
            "8ad5c2e01a204bff96fa9055f0bd5986",
            "2ea79561b8ad458692b45df6dc9cb828",
            "bfb64b82669043dcba5b405b6e03a53b",
            "c16214900cec42ee87af715efed9bafb",
            "9bad0c53e33847b29b5c6df2d22d36bd",
            "ffc41a8f49d2472e96d3a826fa98156b",
            "1a626693a2fd4c5dafa93f13319141b4",
            "1f86f139c54641b8aac58e9312af3848",
            "ce224b6fef4748ceb617f6c8b1422124",
            "ae748fa8633b43a6ba37f695b0df91bb",
            "af7beeb105a64934aefc6fc42516c041",
            "4c85a002f77348f7b3dd376be551841d",
            "a8f9e6c0523c444e8aa7452338a229d7",
            "ddd1e075e45f4809bf7c83feda0ef190",
            "e1ebc8afebae420ca37bbcb5bf51d902",
            "b684dabbd33d49098e3efd496bbf07e9",
            "ea2fa1e0a1744c84991cdd9c756a98a6",
            "11d5b97bf92545e88ede7d79783637a9",
            "1abcc3081902440680a928fcbff0853c",
            "4ca72d6803a34637b0bda35cf6108c09",
            "780a991e007e40b88425cdca9c5181f3",
            "ba04b41b9b9c408daad62959d36918c5",
            "96598f48ed344142a2697b45a7cd6c88",
            "b1f0b9fbf53c4a529f2367e740e3a0df",
            "71e70a4dc12a48809275e4db4c91196e",
            "fe8e49e9507347ebae150b8d88149bf0",
            "9c879257003d44deb2022dc611d9890b",
            "0fcb3c8823244d3a857afed3f2637a6e",
            "3fe5f835ec1e4a2497ffbeec34482635",
            "6b97cfdf5030482992db5fbd094a091d",
            "9dcef288d0c2498e870df9b9b4020e0e",
            "74ef8efeb5474000862ab195da027443",
            "5f9c6891c3984eb493a8cc48a371ac91",
            "b7e0b5d7900e4bf497df06e5fcb05c15",
            "6a2a0816a72e4efab3aec9c0552b8215",
            "b2899cdd2d0c4888b81e59341686ef2c",
            "34a70efe968341c482d5992238bc1b00",
            "aecc2ef8e071447aa894b3b71e0ff317",
            "12b09f2679f74d1eb43f26ab8a9c6ccc",
            "1b7d03f9584048b2b82947443ba90838",
            "9ba4100984154aa7ae37a8ae9a7e6301",
            "29e2f0e737dd4dcfbf95053c4aae886a",
            "0a9ac92b281a4033a5030e0eec7e5d09",
            "26af33d8ffc740bcbe7f1543ff34baf7",
            "40e9bf2a97ac48d68573ada4c720c29e",
            "76e8fec81c7d45b78319cf628b3fc7c8",
            "335df768a96a4a358d7e8d2e1fa5b90a",
            "ca30b08c8e564496be5d8d09e66f7944",
            "a77037c951e24310a0d3b2145c30d343",
            "7a9f4b015cd946289bfc0e7438c36f3c",
            "b9c4ea15b5024086a919e7a98130d975",
            "71bc6c70d9be4aa297e45588940b2499",
            "28be61f457fe47efaa7db8adb182e88b",
            "a879ecada89640e5b8a41611c95b1865",
            "3bdb96752b4b4dcb9cf04a3e37437bf5",
            "142e389cdcf14dc2a3f0757f784fa0bc",
            "e3a6f2e962404316a57f4b5f46200926",
            "6951e7f44a324d349073dadbe484ea94",
            "efd5755cc2094d979b021e159b2682e1",
            "da89189fc4754244b86b482e890822b9",
            "bde67403b98b4eb786fb5749cbf7ca8d",
            "bcf8cfa04f5e4862b493860f347558da",
            "5ab440610e324d93ab4a84408e64ff17",
            "612342dc2af5459d8b1244b6c012a8cb",
            "500e536565584cb1a4ac4ccadd7a3e61",
            "d432a7a1d97d43b8a828f169bf5706d6",
            "d6238af96ad74701abfdb9e7622fe43b",
            "ee51b0fc3ba24169a12fd5a19f1f7c4e",
            "c53a7f268a2048869bf04c6b3e5ef9fe",
            "739c5aeacdd44c09b70ab9101ff4e375",
            "dde5693ca91f43aaa2c85d7d3a8699bc",
            "50bf946b824142a3b8708a8606147eeb",
            "88d35855a37f4076b718ffd8f58c9b12",
            "e2127721c2ea4100bd4dcaa545ed864e",
            "07e8eb41357d48739f059605ba8eaa60",
            "02cba293ea5746baba20ca14befaccfa",
            "63c3e3f53a674dca9afda0e51a3a96f7",
            "4fb19b09b48c4a4892f09d06f2720df4",
            "0ff5507c6fca4aad9641d730c936e462",
            "89a8c070295e45a08d9cf9b14911b154",
            "5b2134142ac34cbbb1316e1bc7f24e0c",
            "bde692351d6d4cc6851c86810b8f84d7",
            "1dffdbd96ded4fa480e7704f52da40a4",
            "22d085c87a1e4a14a9c02d080b18b4a5",
            "11756d3afd0b4d36a1826bf6aaf45797",
            "5ffe2956e839403a87969782bed19dc1",
            "fca343e0121644d4af1ae1f56765a01d",
            "12a72b10a35d4a8f990922367979695d",
            "179f95e33cd8488f8de33f10419b01d5",
            "07bc750ceb9248ed947ee2502016f6f2",
            "d73f758f142741f591b191ae657a2115",
            "1a2e1f5fa1314b06b7c041f8018ff300",
            "f4374145d3ac4110a7ee5f614522229d",
            "0dceb81b7d7d4bafbf442268f70b2a81",
            "5d4aecdf73f241b98b8f8c5e6a37e045",
            "de4d9c5ac59f41fab01f9c3c80e48d1d",
            "4a51088689f74eaabf8d2e8ba718744c",
            "8a3dcf5e77f64869a86b10b2e8630b69",
            "7430bf6096c24b9daa0ce838948599c6",
            "7c14cfe8640d42c9a77293ccfac1a567",
            "1cf64145eb1b4274988815290a424b10",
            "4bb2b04459b747c7a27e2e77bf9dce25"
          ]
        },
        "id": "PhYEUcCCMmaE",
        "outputId": "95384055-9fa5-4733-995d-af445e526367",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b8de767a0464eae91e3e5f913ad0aea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc0b3555e82b444da632955d57f2ae84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b3dd170a42c49ac9ae2466a9914f9bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "716f2f8e25784cf1850145427c63f011"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd28a781d9e9468ab8eb043b65faaa83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ef2b126950749d1824fd2370090a8f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "074ba694002b4695aade1837db969e86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "517b03ec9c1e4b62a8ddc6e85c15de4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f4d1b1b37f4486cb7bfa0d158c1439d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74f8701ca12c4125acb9e28146899e88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c16214900cec42ee87af715efed9bafb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1ebc8afebae420ca37bbcb5bf51d902"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe8e49e9507347ebae150b8d88149bf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34a70efe968341c482d5992238bc1b00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca30b08c8e564496be5d8d09e66f7944"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efd5755cc2094d979b021e159b2682e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "739c5aeacdd44c09b70ab9101ff4e375"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b2134142ac34cbbb1316e1bc7f24e0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a2e1f5fa1314b06b7c041f8018ff300"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GraphSAGEClassifier(nn.Module):\n",
        "    def __init__(self, in_c, hidden_c, out_c, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.sage1 = SAGEConv(in_c, hidden_c)\n",
        "        self.sage2 = SAGEConv(hidden_c, hidden_c)\n",
        "        self.sage3 = SAGEConv(hidden_c, hidden_c)\n",
        "        self.attn_weights = nn.Parameter(torch.randn(hidden_c, 1))\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_c + 9, 64),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, out_c)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_ids_batch, heuristics, graph):\n",
        "        x, edge_index = graph.x, graph.edge_index\n",
        "\n",
        "        # 1) Three GNN layers\n",
        "        x = self.sage1(x, edge_index).relu()\n",
        "        x = self.sage2(x, edge_index).relu()\n",
        "        x = self.sage3(x, edge_index).relu()\n",
        "\n",
        "        node_feats       = []\n",
        "        attn_scores_list = []\n",
        "\n",
        "        for node_ids in node_ids_batch:\n",
        "            # Gather the node embeddings for this sample\n",
        "            embeds = x[node_ids]\n",
        "\n",
        "            scores = embeds @ self.attn_weights\n",
        "\n",
        "            attn = torch.softmax(scores, dim=0)\n",
        "\n",
        "            node_feats.append((attn * embeds).sum(dim=0))\n",
        "\n",
        "            attn_scores_list.append(attn.squeeze(1).cpu().detach().numpy())\n",
        "\n",
        "        graph_feats = torch.stack(node_feats)\n",
        "\n",
        "        combined = torch.cat([graph_feats, heuristics.to(DEVICE)], dim=1)\n",
        "\n",
        "        logits = self.mlp(combined)\n",
        "\n",
        "        return logits, attn_scores_list\n",
        "\n",
        "\n",
        "gnn = GraphSAGEClassifier(in_c=1, hidden_c=128, out_c=3).to(DEVICE)\n",
        "gnn.load_state_dict(torch.load(GNN_WEIGHTS_PATH, map_location=DEVICE))\n",
        "gnn.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Y5tEqqFO_s",
        "outputId": "e1d44b3e-a0d4-45ef-8919-9d2123da4480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GraphSAGEClassifier(\n",
              "  (sage1): SAGEConv(1, 128, aggr=mean)\n",
              "  (sage2): SAGEConv(128, 128, aggr=mean)\n",
              "  (sage3): SAGEConv(128, 128, aggr=mean)\n",
              "  (mlp): Sequential(\n",
              "    (0): Linear(in_features=137, out_features=64, bias=True)\n",
              "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Linear(in_features=64, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntz622bET_-u",
        "outputId": "5feab5b3-d82d-4992-b0bc-00cf4a016768",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_node_attention_per_image(result, top_k=10):\n",
        "    node_attn_sorted = sorted(\n",
        "        result[\"node_attention\"], key=lambda x: x[\"weight\"], reverse=True\n",
        "    )[:top_k]\n",
        "    nodes   = [e[\"node\"] for e in node_attn_sorted]\n",
        "    weights = [e[\"weight\"] for e in node_attn_sorted]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.barh(nodes, weights)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xlabel(\"Attention Weight\")\n",
        "    ax.set_title(\"Top-10 Node Attention\")\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def hazard_detection_app(image_pil):\n",
        "    # 1. BLIP captioning\n",
        "    img = image_pil.convert(\"RGB\")\n",
        "    inputs = blip_processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
        "    out_ids = blip.model.generate(**inputs, max_new_tokens=50)\n",
        "    caption = blip_processor.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # 2. Extract features & triplets\n",
        "    feat = compute_features(caption)\n",
        "    trip = extract_triplets(caption)\n",
        "    nodes_idx = [\n",
        "        node_to_idx[n]\n",
        "        for u, v, *rest in trip\n",
        "        for n in (u, v)\n",
        "        if n in node_to_idx\n",
        "    ]\n",
        "    if not nodes_idx:\n",
        "        nodes_idx = [0]\n",
        "\n",
        "    # 3. GNN forward → logits + per-node attention\n",
        "    with torch.no_grad():\n",
        "        logits, attn_scores_list = gnn([nodes_idx], feat, data_graph)\n",
        "    label_idx = logits.argmax(dim=1).item()\n",
        "    rating = [\"LOW\", \"MEDIUM\", \"HIGH\"][label_idx]\n",
        "\n",
        "    # 4. Build node_attention list\n",
        "    node_attention = [\n",
        "        {\"node\": n, \"weight\": w}\n",
        "        for (u, v, *_), w in zip(trip, attn_scores_list[0])\n",
        "        for n in (u, v)\n",
        "        if n in node_to_idx\n",
        "    ]\n",
        "\n",
        "    # 5. Build attention plot\n",
        "    result = {\n",
        "        \"filename\": \"<upload>\",\n",
        "        \"caption\": caption,\n",
        "        \"predicted_rating\": rating,\n",
        "        \"node_attention\": node_attention\n",
        "    }\n",
        "    fig = plot_node_attention_per_image(result, top_k=10)\n",
        "\n",
        "    return caption, rating, fig\n",
        "\n",
        "# —— Gradio UI layout ——\n",
        "custom_theme = gr.themes.Base(font=[\"Arial\",\"sans-serif\"]).set(\n",
        "    body_text_size=\"16px\"\n",
        ")\n",
        "\n",
        "with gr.Blocks(theme=custom_theme, css=\"\"\"\n",
        "    .gradio-container { max-width: 1200px; margin: auto; }\n",
        "\"\"\") as demo:\n",
        "    gr.Markdown(\"## 🚧 Single‐Image Hazard Detection\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            inp_img = gr.Image(type=\"pil\", label=\"📷 Upload Road Scene\")\n",
        "            btn_run = gr.Button(\"Run Inference\")\n",
        "        with gr.Column(scale=1):\n",
        "            out_caption = gr.Textbox(label=\"📝 BLIP Caption\", lines=3)\n",
        "            out_hazard  = gr.Textbox(label=\"⚠️ Predicted Hazard Level\", lines=1)\n",
        "            out_plot    = gr.Plot(label=\"🔍 Top-10 Node Attention\")\n",
        "\n",
        "    btn_run.click(\n",
        "        fn=hazard_detection_app,\n",
        "        inputs=[inp_img],\n",
        "        outputs=[out_caption, out_hazard, out_plot]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "RfNw6nhkT1hJ",
        "outputId": "55fadb50-2462-4b4b-fa28-0ff72979fdbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2e8ed4a2aa919dea54.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2e8ed4a2aa919dea54.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}